{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59edd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,sys\n",
    "sys.path.append(os.path.abspath(\".\")) \n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"aside\")) # to access aside.experiments \n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from utils.model_utils import load_model\n",
    "from vllm import SamplingParams\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # to suppress warning message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bd546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 11:51:44 [utils.py:328] non-default args: {'max_model_len': 32768, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'enable_chunked_prefill': True, 'model': '/mnt/disk1/xulin/models/Qwen3-8B'}\n",
      "INFO 10-09 11:51:44 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 10-09 11:51:44 [__init__.py:1815] Using max model len 32768\n",
      "INFO 10-09 11:51:44 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 10-09 11:51:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:50 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:50 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/mnt/disk1/xulin/models/Qwen3-8B', speculative_config=None, tokenizer='/mnt/disk1/xulin/models/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/disk1/xulin/models/Qwen3-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1009 11:51:52.075006004 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:52 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m WARNING 10-09 11:51:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:52 [gpu_model_runner.py:2338] Starting to load model /mnt/disk1/xulin/models/Qwen3-8B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:52 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:52 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.29it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.44it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.44it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.61it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:56 [default_loader.py:268] Loading weights took 3.14 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:51:56 [gpu_model_runner.py:2392] Model loading took 15.2683 GiB and 3.329125 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:02 [backends.py:539] Using cache directory: /home/wjyeo/.cache/vllm/torch_compile_cache/5c00c4fc42/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:02 [backends.py:550] Dynamo bytecode transform time: 5.43 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:04 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.165 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:05 [monitor.py:34] torch.compile takes 5.43 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:06 [gpu_worker.py:298] Available KV cache memory: 90.83 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:06 [kv_cache_utils.py:864] GPU KV cache size: 661,424 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:06 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 20.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:10 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:10 [gpu_worker.py:391] Free memory on device (139.21/139.81 GiB) on startup. Desired GPU memory utilization is (0.8, 111.85 GiB). Actual usage is 15.27 GiB for weight, 5.68 GiB for peak activation, 0.07 GiB for non-torch memory, and 0.77 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=96544422195` to fit into requested memory, or `--kv-cache-memory=125921838592` to fully utilize gpu memory. Current kv cache memory in use is 97532180787 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3731346)\u001b[0;0m INFO 10-09 11:52:10 [core.py:218] init engine (profile, create kv cache, warmup model) took 13.73 seconds\n",
      "INFO 10-09 11:52:10 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-09 11:52:10 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 10-09 14:20:34 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"Qwen/Qwen3-8B\"\n",
    "model_path = '/mnt/disk1/xulin/models/Qwen3-8B'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "use_vllm = True\n",
    "model,tokenizer,is_aside = load_model(model_path,use_vllm=use_vllm,dtype=torch.bfloat16,vllm_kwargs = {'gpu_memory_utilization':0.8,'enable_chunked_prefill':True}) # load model if generating the toucan dpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5691b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6d16fb7dbc4c34a2dd129e3e6d035b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('Agent-Ark/Toucan-1.5M','SFT',split = 'train').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f108c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_json_loads(value): # THE JSON tool call labels are messy.\n",
    "    \"\"\"Try to parse value as JSON, otherwise return as-is.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed = json.loads(value)\n",
    "            # Recursively process the parsed result as well\n",
    "            return recursively_fix(parsed)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "def recursively_fix(obj):\n",
    "    \"\"\"Recursively decode nested JSON strings inside dicts/lists.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursively_fix(try_json_loads(v)) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursively_fix(try_json_loads(x)) for x in obj]\n",
    "    else:\n",
    "        return try_json_loads(obj)\n",
    "\n",
    "def fix_tool_response(tool_response):\n",
    "    tool_response = deepcopy(tool_response)\n",
    "    tool_response = ast.literal_eval(tool_response)\n",
    "    tool_response = recursively_fix(tool_response)\n",
    "    return json.dumps(tool_response, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82779230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119287/119287 [00:24<00:00, 4807.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid cat no_tool_use: 40000 samples\n",
      "Valid cat single-turn-original: 28254 samples\n",
      "Valid cat single-turn-diversify: 15790 samples\n",
      "Valid cat multi-turn: 35243 samples\n",
      "Total samples with assistant tool calls: 0 / 119287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## reformat the dataset\n",
    "toolcall_format = \"<tool_call>\\n{tool}\\n</tool_call>\\n\"\n",
    "\n",
    "mode = 'dpo' # set as sft or dpo\n",
    "sample_single_assistant_turns = 'qwen' in tokenizer.name_or_path.lower() and mode == 'sft' # Qwen only generate tool calls, so we can train on single assistant turns. Also, we only do this for sft since dpo, we will generate the data.\n",
    "\n",
    "# Get the length.\n",
    "def get_len(sample):\n",
    "    convs = sample['conversations']\n",
    "    formatted = tool_prompt_format(convs,sample['tools'],tokenizer)\n",
    "    sample['len'] =len(tokenizer.encode(formatted))\n",
    "    return sample\n",
    "\n",
    "cat_ds = defaultdict(list)\n",
    "invalid_cats = defaultdict(int)\n",
    "\n",
    "\n",
    "for sample in tqdm(ds,total = len(ds)):\n",
    "    try: # have to be able to parse both of them.\n",
    "        tools = json.loads(sample['tools'])\n",
    "        messages = json.loads(sample['messages'])\n",
    "    except:\n",
    "        invalid_cats['parsing_error'] += 1\n",
    "        continue\n",
    "    \n",
    "    cat = sample['subset_name']\n",
    "    roles = [m['role'] for m in messages]\n",
    "    if not any([r == 'tool_response' for r in roles]): # no tool roles\n",
    "        invalid_cats['no_tool_use'] += 1\n",
    "        continue\n",
    "    \n",
    "    structured_msg = []\n",
    "    curr_tool_call = ''\n",
    "    assistant_start = False\n",
    "    valid_assistant_turns = [] # We add valid assistant turns with the option of choosing any random turn to only do SFT on that turn, the issue is that in Qwen, the no-thinking mode causes extra tokens <think> and </think> to be added only to the last turn, thus we if train on all turns, usually only the tool turns are earlier and do not have these <think> tokens, thus there will be a mismatch.\n",
    "    tool_started = False # we try to optimize for the case where only when there is a tool return, then we count the assistant turns. (not valid in single-turn)\n",
    "    for msg_id,m in enumerate(messages): # we ignore the assistant role since Qwen do not generate any text other than the MCP tools\n",
    "        if m['role'] == 'tool_call':\n",
    "            tool_response = fix_tool_response(m['content'])\n",
    "            curr_tool_call += toolcall_format.format(tool = tool_response)\n",
    "        else:\n",
    "            if curr_tool_call != '': # if the next msg is not tool_call, we dump the curr_tool_call\n",
    "                structured_msg.append({'role':'assistant','content':curr_tool_call.strip()})\n",
    "                curr_tool_call = ''\n",
    "                if tool_started:\n",
    "                    valid_assistant_turns.append(len(structured_msg)-1)\n",
    "            if m['role'] == 'user':\n",
    "                structured_msg.append(m)\n",
    "            elif m['role'] == 'tool_response':\n",
    "                tool_started = True\n",
    "                structured_msg.append({'role':'tool','content':m['content']})\n",
    "            elif m['role'] == 'assistant': # if is assistant, we only keep it if it is the last msg or if the next msg is not tool_call.\n",
    "                if msg_id == len(messages) - 1 or messages[msg_id + 1]['role'] != 'tool_call':\n",
    "                    structured_msg.append(m)\n",
    "                    if tool_started:\n",
    "                        valid_assistant_turns.append(len(structured_msg)-1)\n",
    "    if sample_single_assistant_turns:\n",
    "        if len(valid_assistant_turns) > 1:\n",
    "            chosen_turn = np.random.choice(valid_assistant_turns)\n",
    "            structured_msg = structured_msg[:chosen_turn+1]\n",
    "            assert structured_msg[-1]['role'] == 'assistant', \"Last message must be assistant!\"\n",
    "        if '<tool_call>' in structured_msg[-1]['content']: # split the last turn to either involving a tool call or jsut a sumamry sentence else we use the original cat\n",
    "            cat_label = 'tool'\n",
    "        else:\n",
    "            cat_label = 'non_tool'\n",
    "    else:\n",
    "        cat_label = sample['subset_name']\n",
    "    cat_ds[cat_label].append(\n",
    "        {'conversations':structured_msg,\n",
    "         'tools':tools,}\n",
    "    )\n",
    "\n",
    "for k in invalid_cats:\n",
    "    print(f\"Invalid cat {k}: {invalid_cats[k]} samples\")\n",
    "\n",
    "for k in cat_ds:\n",
    "    print(f\"Valid cat {k}: {len(cat_ds[k])} samples\")\n",
    "\n",
    "print (f\"Total samples with assistant tool calls: {len(cat_ds['tool'])} / {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66dd0103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing input lengths: 100%|██████████████████████████| 28254/28254 [00:19<00:00, 1472.00it/s]\n",
      "Computing input lengths: 100%|██████████████████████████| 15790/15790 [00:10<00:00, 1574.32it/s]\n",
      "Computing input lengths: 100%|███████████████████████████| 35243/35243 [00:36<00:00, 967.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use multi process to get the lengths\n",
    "cat_ds_w_len = {k:async_process(get_len,cat_ds[k],workers=32, msg=\"Computing input lengths\") for k in cat_ds}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6fe075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭──────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Cat single-turn-original: before filtering 28254, after filtering 22863                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Cat single-turn-original: before filtering 28254, after filtering 22863                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭──────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Cat single-turn-diversify: before filtering 15790, after filtering 13314                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Cat single-turn-diversify: before filtering 15790, after filtering 13314                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭──────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Cat multi-turn: before filtering 35243, after filtering 21562                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Cat multi-turn: before filtering 35243, after filtering 21562                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = 4096\n",
    "cat_ds_valid = {k:[s for s in cat_ds_w_len[k] if s['len'] <= MAX_LENGTH] for k in cat_ds_w_len}\n",
    "for k in cat_ds_w_len:\n",
    "    pprint (f\"Cat {k}: before filtering {len(cat_ds_w_len[k])}, after filtering {len(cat_ds_valid[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4497f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# we should balance it out, since the dataset has more non-tool samples. we take an equal amount of tool and non-tool samples.\n",
    "if sample_single_assistant_turns:\n",
    "    min_cat_size = min([len(cat_ds_valid[k]) for k in cat_ds_valid])\n",
    "    for k in cat_ds_valid:\n",
    "        cat_ds_valid[k] = np.random.permutation(cat_ds_valid[k])[:min_cat_size].tolist()\n",
    "\n",
    "concatenated_ds = sum(list(cat_ds_valid.values()),[])\n",
    "random_ids = np.random.permutation(len(concatenated_ds))\n",
    "val_size = 500\n",
    "train_ids = random_ids[val_size:]\n",
    "val_ids = random_ids[:val_size]\n",
    "\n",
    "train_ds = [concatenated_ds[i] for i in train_ids]\n",
    "val_ds = [concatenated_ds[i] for i in val_ids]\n",
    "\n",
    "print (f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")\n",
    "\n",
    "with open('/mnt/disk1/wjyeo/data/toucan_train.json','w') as f:\n",
    "    json.dump(train_ds,f,indent = 4)\n",
    "with open('/mnt/disk1/wjyeo/data/toucan_val.json','w') as f:\n",
    "    json.dump(val_ds,f,indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f74db2",
   "metadata": {},
   "source": [
    "# Create adversarial samples\n",
    "\n",
    "1. To ease the insertion of attack, we find tool_response that just returns a string as data points to insert the attack\n",
    "2. We first find possible attacks : for each sample, get the user instruction, generate on-policy tool call for that. (this will be the rejected response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a2a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid cat single-turn-original: 28254 samples\n",
      "Valid cat single-turn-diversify: 15790 samples\n",
      "Valid cat multi-turn: 35243 samples\n",
      "Valid cat tool: 0 samples\n"
     ]
    }
   ],
   "source": [
    "for k in cat_ds:\n",
    "    print(f\"Valid cat {k}: {len(cat_ds[k])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00a2ad",
   "metadata": {},
   "source": [
    "**We need to balance between tool calling and non-tool calling chosen/rejected**\n",
    "\n",
    "The reason is because rejected is always tool calling, if we take chosen responses that aren't tool calling, i.e. responding the user's query using the retrieved tool response, it will result in an unintended behavior where the model learns to not use tools!\n",
    "\n",
    "And since we can only take inputs after the 1st tool response, this automatically filters out single-turn samples that do not have tool calls after the initial and only tool call. Thus for multi-tool samples, we only take after the 1st tool call and we can create multiple truncated conversations of the same sample whenver we encounter the pattern: \"Editable Tools\" -> assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae71d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_words(d):\n",
    "    \"\"\"\n",
    "    Recursively find the (key_path, value, word_count) where the value\n",
    "    is the longest string by word count in any nested dict/list structure.\n",
    "    Supports paths like 'a.b[2].c'.\n",
    "    \"\"\"\n",
    "    max_key_path, max_value, max_words = None, None, 0\n",
    "\n",
    "    def recurse(obj, path=\"\"):\n",
    "        nonlocal max_key_path, max_value, max_words\n",
    "\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                new_path = f\"{path}.{k}\" if path else k\n",
    "                recurse(v, new_path)\n",
    "\n",
    "        elif isinstance(obj, list):\n",
    "            for idx, item in enumerate(obj):\n",
    "                new_path = f\"{path}[{idx}]\" if path else f\"[{idx}]\"\n",
    "                recurse(item, new_path)\n",
    "\n",
    "        elif isinstance(obj, str):\n",
    "            word_count = len(obj.split())\n",
    "            if word_count > max_words:\n",
    "                max_key_path, max_value, max_words = path, obj, word_count\n",
    "\n",
    "    recurse(d)\n",
    "    return max_key_path, max_value, max_words\n",
    "\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "def set_by_path(d, path, new_value):\n",
    "    \"\"\"\n",
    "    Set a nested value in a dict/list structure using a path like:\n",
    "    'a.b[0].c'. Works in place and returns the modified object.\n",
    "    \"\"\"\n",
    "    # Break path into segments:  e.g. \"a.b[0].c\" → [\"a\", \"[0]\", \"c\"]\n",
    "    parts = re.findall(r'\\w+|\\[\\d+\\]', path)\n",
    "\n",
    "    current = d\n",
    "    for part in parts[:-1]:\n",
    "        if part.startswith('[') and part.endswith(']'):\n",
    "            idx = int(part[1:-1])\n",
    "            current = current[idx]\n",
    "        else:\n",
    "            current = current[part]\n",
    "\n",
    "    last = parts[-1]\n",
    "    if last.startswith('[') and last.endswith(']'):\n",
    "        idx = int(last[1:-1])\n",
    "        current[idx] = new_value\n",
    "    else:\n",
    "        current[last] = new_value\n",
    "\n",
    "    return d\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58193c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool_response(tool_resp):\n",
    "    tool_resp = fix_tool_response(tool_resp)\n",
    "    try:\n",
    "        json_dict = json.loads(tool_resp)\n",
    "        if not isinstance(json_dict, dict): # if it is not a dict, it has to be at least 5 words to be considered as text\n",
    "            if len(tool_resp.split()) > 5:\n",
    "                return {'type':'text', 'content':tool_resp}\n",
    "            else:\n",
    "                return {}\n",
    "        else: # is a dict, we traverse the dict and try to find the value with the highest number of words, if it is < 5, we discard the whole thing, else we save the path so that can we can directly use that path to edit the text\n",
    "            key_path, longest_text, word_count = find_most_words(json_dict)\n",
    "            if word_count > 5:\n",
    "                return {'type':'dict', 'content':json_dict, 'key_path':key_path,'text':longest_text}\n",
    "            else:\n",
    "                return {}\n",
    "            \n",
    "    except:\n",
    "        if len(tool_resp.split()) > 5: # at least 5 splitable words, if yes, we take until the assistant turn\n",
    "            return {'type':'text', 'content':tool_resp}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def retrieve_adversarial_samples(examples): # can have multiple of the sample example if we encounter the pattern of editable tool -> assistant.\n",
    "    valid_examples = []\n",
    "    for sample in tqdm(examples,total = len(examples)):\n",
    "        last_assistant_idx = -1\n",
    "        for i,msg in enumerate(sample['conversations']):\n",
    "            valid_tool_turns = []\n",
    "            if i <= last_assistant_idx: # we want to skip the earlier turns that have been already added as examples\n",
    "                continue\n",
    "            if msg['role'] == 'tool': # once found a tool, look for the next assistant turn\n",
    "                tool_resp = msg['content']\n",
    "                if len(parse_tool_response(tool_resp)) > 0: # is an editable tool response\n",
    "                    valid_tool_turns.append(i)\n",
    "                    for j in range(i+1,len(sample['conversations'])): \n",
    "                        if sample['conversations'][j]['role'] == 'assistant':\n",
    "                            valid_examples.append({ # keep the assistant turn as we want to use it for the completion attack\n",
    "                                'conversations':sample['conversations'][:j+1],\n",
    "                                'tools':sample['tools'],\n",
    "                                'valid_tool_turns':valid_tool_turns, # keep track of which tool turns are valid for attack insertion\n",
    "                            })\n",
    "                            last_assistant_idx = j\n",
    "                            break # if is not a valid assistant turn, we stop here and wait for the next tool call\n",
    "                        elif sample['conversations'][j]['role'] == 'tool' and len(parse_tool_response(sample['conversations'][j]['content'])): # keep in mind that there might be multiple tool calls that return string, so we keep track of those valid ones\n",
    "                            valid_tool_turns.append(j)\n",
    "\n",
    "\n",
    "    return valid_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a4857c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined ds size: 79287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79287/79287 [00:03<00:00, 22008.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid adversarial samples: 102228\n",
      "Deduplicated Valid adversarial samples: 75492\n"
     ]
    }
   ],
   "source": [
    "combined_ds = sum(list(cat_ds.values()),[])\n",
    "print (f\"Total combined ds size: {len(combined_ds)}\")\n",
    "valid_adversarial_ds = retrieve_adversarial_samples(combined_ds)\n",
    "print(f\"Valid adversarial samples: {len(valid_adversarial_ds)}\")\n",
    "\n",
    "def dedupe_examples(examples):\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for ex in examples:\n",
    "        # canonicalize the conversations slice to a JSON string for hashing\n",
    "        key = json.dumps(ex['conversations'], sort_keys=True, ensure_ascii=False)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(ex)\n",
    "    return uniq\n",
    "\n",
    "valid_adversarial_ds = dedupe_examples(valid_adversarial_ds)\n",
    "print(f\"Deduplicated Valid adversarial samples: {len(valid_adversarial_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59949f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79287/79287 [00:00<00:00, 602668.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "attack_samples = []\n",
    "for i,sample in tqdm(enumerate(combined_ds),total = len(combined_ds)):\n",
    "    for j,msg in enumerate(sample['conversations']):\n",
    "        if msg['role'] == 'user':\n",
    "            # next turn should be assistant and not tool response (will be weird)\n",
    "            if sample['conversations'][j+1]['role'] != 'assistant':\n",
    "                break\n",
    "            else:\n",
    "                attack_samples.append({\n",
    "                    'conversations':sample['conversations'][:j+1], # dont include the assistant turn\n",
    "                    'tools':sample['tools'],\n",
    "                })\n",
    "                break\n",
    "print (len(attack_samples))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "955a548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing input lengths for adv ds: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75492/75492 [01:02<00:00, 1204.14it/s]\n",
      "Computing input lengths for attack samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 75532/75532 [00:24<00:00, 3138.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid adversarial samples in cat after length filtering: 45894 from 75492\n",
      "Number of valid attack samples after length filtering: 75532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input len is not too long\n",
    "MAX_LENGTH = 4096-512\n",
    "adv_ds_w_len = async_process(get_len,valid_adversarial_ds,workers=64, msg=\"Computing input lengths for adv ds\")\n",
    "attack_samples_w_len = async_process(get_len,attack_samples,workers=64, msg=\"Computing input lengths for attack samples\")\n",
    "\n",
    "valid_len_adversarial_ds = [s for s in adv_ds_w_len if s['len'] <= MAX_LENGTH]\n",
    "print (f\"Number of valid adversarial samples in cat after length filtering: {len(valid_len_adversarial_ds)} from {len(adv_ds_w_len)}\")\n",
    "\n",
    "valid_adversarial_ds = valid_len_adversarial_ds\n",
    "attack_samples = [s for s in attack_samples_w_len if s['len'] <= MAX_LENGTH]\n",
    "print (f\"Number of valid attack samples after length filtering: {len(attack_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b2627d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "OTHER_DELM_TOKENS = {\n",
    "    'mark': ['{s}', '|{s}|', '<{s}>', '[{s}]', '<|{s}|>', '[|{s}|]', '<[{s}]>', '\\'\\'\\'{s}\\'\\'\\'', '***{s}***'],\n",
    "    'inst': ['Command', 'Rule', 'Prompt', 'Task'],\n",
    "    'inpt': ['Data', 'Context', 'Text'],\n",
    "    'resp': ['Output', 'Answer', 'Reply'],\n",
    "    'user': ['', 'Prompter ', 'User ', 'Human '],\n",
    "    'asst': ['', 'Assistant ', 'Chatbot ', 'Bot ', 'GPT ', 'AI '],\n",
    "}\n",
    "\n",
    "def create_injection_for_completion(response, instruction):\n",
    "    mark = np.random.choice(OTHER_DELM_TOKENS['mark']) + ':'\n",
    "    \n",
    "    def sample_delm(delm_name):\n",
    "        role_name = 'user' if (delm_name == 'inst' or delm_name == 'inpt') else 'asst'\n",
    "        role = np.random.choice(OTHER_DELM_TOKENS[role_name]) \n",
    "        delm = np.random.choice(OTHER_DELM_TOKENS[delm_name])\n",
    "        p = np.random.rand()\n",
    "        if p < 1/3: return (role + delm).upper()\n",
    "        elif p < 2/3: return (role + delm).lower()\n",
    "        else: return role + delm\n",
    "    \n",
    "    text = mark.format(s=sample_delm('resp')) + '\\n\\n' + response \n",
    "    text += '\\n\\n' + mark.format(s=sample_delm('inst')) + '\\n\\n' + instruction\n",
    "    return text\n",
    "\n",
    "def jload(f, mode=\"r\", num_samples=None):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    if isinstance(obj, (dict, list)): json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str): f.write(obj)\n",
    "    else: raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def calculate_length_for_preference_dataset(dataset, tokenizer):\n",
    "    chosen_input_ids = tokenizer([d['chosen'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "    rejected_input_ids = tokenizer([d['rejected'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    chosen_lengths = np.array([len(prompt) for prompt in chosen_input_ids])\n",
    "    rejected_lengths = np.array([len(prompt) for prompt in rejected_input_ids])\n",
    "    prompt_and_label_lengths = np.maximum(chosen_lengths,rejected_lengths)\n",
    "\n",
    "    print('Input+Output model_max_length (98%, 99%, 99.5%, 99.9%):', np.percentile(prompt_and_label_lengths, [95, 99, 99.5, 99.9]))\n",
    "    print (f'Mean: {(np.mean(chosen_lengths) + np.mean(rejected_lengths))/2:.2f} Num > 2048: {np.sum(prompt_and_label_lengths>2048)} / {len(prompt_and_label_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f4aea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45894/45894 [04:47<00:00, 159.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate suitable attacks, suitable means that the instruction should cause the model to call a tool, else abandon those samples.\n",
    "Chosen input are samples from valid_adversarial_ds and rejected inputs are from attack_samples\n",
    "\"\"\" \n",
    "\n",
    "def find_instruction(convs):\n",
    "    for msg in convs:\n",
    "        if msg['role'] == 'user':\n",
    "            return msg['content']\n",
    "    return None\n",
    "\n",
    "preference_input = []\n",
    "for sample in tqdm(valid_adversarial_ds,total = len(valid_adversarial_ds)):\n",
    "    current_sample = deepcopy(sample)\n",
    "    injected_sample = np.random.choice(attack_samples,size=  1,replace=False)[0]\n",
    "\n",
    "    clean_instruction = find_instruction(current_sample['conversations'])\n",
    "    injected_instruction = find_instruction(injected_sample['conversations'])\n",
    "\n",
    "    while injected_instruction == clean_instruction: # make sure that the injected instruction is different from the clean one\n",
    "        injected_sample = np.random.choice(attack_samples,size=  1,replace=False)[0]\n",
    "        injected_instruction = find_instruction(injected_sample['conversations'])\n",
    "    valid_tool_turns = current_sample['valid_tool_turns']\n",
    "\n",
    "    clean_convs = deepcopy(current_sample['conversations'])\n",
    "    # turn_to_inject = valid_tool_turns[-1] # inject at the last valid tool turn\n",
    "    is_text_tools = []\n",
    "    for valid_t in valid_tool_turns:\n",
    "        parsed_tool = parse_tool_response(clean_convs[valid_t]['content'])\n",
    "        if len(parsed_tool) > 0 and parsed_tool['type'] == 'text':\n",
    "            is_text_tools.append(valid_t)\n",
    "    if len(is_text_tools) > 0: # prioritize text tools\n",
    "        turn_to_inject = np.random.choice(is_text_tools,size=1,replace=False)[0] # randomly choose a valid tool turn to inject\n",
    "    else: # else choose any valid tool turn\n",
    "        turn_to_inject = np.random.choice(valid_tool_turns,size=1,replace=False)[0] # randomly choose a valid tool turn to inject\n",
    "    injected_data = parse_tool_response(clean_convs[turn_to_inject]['content']) # either a string or a dict\n",
    "    if injected_data['type'] == 'dict':\n",
    "        injected_data_text = injected_data['text']\n",
    "        injected_data_path = injected_data['key_path']\n",
    "        injected_data_dict = injected_data['content']\n",
    "    else:\n",
    "        injected_data_text = injected_data['content']\n",
    "        injected_data_path = None\n",
    "    \n",
    "    ## do the injection here on the injected_data_text\n",
    "    if injected_data_text[-1] not in ['.','!','?']:\n",
    "        injected_data_text += '.'\n",
    "    if np.random.rand() < 0.9:  # 90% Straightforward Attack, 10% Completion Attack\n",
    "        assert clean_convs[turn_to_inject]['role'] == 'tool', \"Should be tool turn\"\n",
    "        injected_data_text = injected_instruction + \" \" + injected_data_text if np.random.rand() < 0.5 else injected_data_text + \" \" + injected_instruction\n",
    "    else: # completion attack, use the actual output\n",
    "        original_response = clean_convs[-1]['content'] # the assistant turn\n",
    "        injected_data_text += '\\n\\n' + create_injection_for_completion(original_response, injected_instruction)\n",
    "    \n",
    "    ## directly replace the text or replace the value at the path\n",
    "    if injected_data_path is not None: # if is a dict, we need to replace the value at the path\n",
    "        try:\n",
    "            injected_data_dict = set_by_path(injected_data_dict, injected_data_path, injected_data_text)\n",
    "        except:\n",
    "            continue # if cannot set by path, we skip this sample\n",
    "        clean_convs[turn_to_inject]['content'] = json.dumps(injected_data_dict, ensure_ascii=False)\n",
    "    else:\n",
    "        clean_convs[turn_to_inject]['content'] = injected_data_text\n",
    "        \n",
    "\n",
    "    if clean_convs[-1]['role'] == 'assistant':\n",
    "        clean_convs = clean_convs[:-1] # take away the assistant turn, as we want the model to generate it\n",
    "\n",
    "    assert all([isinstance(t,dict) for t in current_sample['tools']]), \"Tools should be dicts\"\n",
    "    assert all([isinstance(t,dict) for t in injected_sample['tools']]), \"Tools should be dicts\"\n",
    "    preference_input.append(\n",
    "        {'conversations':clean_convs,\n",
    "        'tools':current_sample['tools'],\n",
    "        'chosen_input':sample,# includes the assistant, remember to take away the assistant turn\n",
    "        'chosen_tools':current_sample['tools'],\n",
    "        'rejected_input':injected_sample, \n",
    "        'rejected_tools':injected_sample['tools'],\n",
    "        }\n",
    "    )\n",
    "print (len(preference_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ea245fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid samples for preference data: 15888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68785836aff4cc5bcf3bc413b079912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/15888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc1df52d702499699f4de103dd30b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input+Output model_max_length (98%, 99%, 99.5%, 99.9%): [511. 512. 512. 512.]\n",
      "Mean: 94.51 Num > 2048: 0 / 15388\n",
      "Input+Output model_max_length (98%, 99%, 99.5%, 99.9%): [512. 512. 512. 512.]\n",
      "Mean: 96.22 Num > 2048: 0 / 500\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=2048, stop=tokenizer.eos_token)\n",
    "\n",
    "# First generate chosen responses (take 80% of samples where the chosen is a tool call, 20% where it is normal text)\n",
    "for sample in preference_input:\n",
    "    chosen_input = sample['chosen_input']['conversations'][:-1] # take away the assistant turn\n",
    "    assert chosen_input[-1]['role'] != 'assistant'\n",
    "    formatted_chosen = tool_prompt_format(chosen_input,sample['chosen_tools'],tokenizer)\n",
    "    sample['formatted_chosen'] = formatted_chosen\n",
    "    rejected_input = sample['rejected_input']['conversations']\n",
    "    formatted_rejected = tool_prompt_format(rejected_input,sample['rejected_tools'],tokenizer)\n",
    "    sample['formatted_rejected'] = formatted_rejected\n",
    "\n",
    "# First generate the chosen responses\n",
    "chosen_inputs = [s['formatted_chosen'] for s in preference_input]\n",
    "chosen_outputs = vllm_generate(model, chosen_inputs, sampling_params,use_tqdm=True)\n",
    "\n",
    "for sample, output in zip(preference_input, chosen_outputs):\n",
    "    sample['chosen'] = output\n",
    "\n",
    "def check_tool_use(output):\n",
    "    return '<tool_call>' in output and '</tool_call>' in output\n",
    "\n",
    "samples_with_chosen_tool_use_id = [i for i,s in enumerate(chosen_outputs) if check_tool_use(s)] # then take these samples and generate the rejected responses\n",
    "print (f\"Total samples with chosen tool use: {len(samples_with_chosen_tool_use_id)} / {len(chosen_outputs)}\")\n",
    "samples_without_chosen_tool_use_id = [i for i in range(len(chosen_outputs)) if i not in samples_with_chosen_tool_use_id]\n",
    "\n",
    "tool_split = 0.8\n",
    "\n",
    "tool_use_samples = [preference_input[i] for i in samples_with_chosen_tool_use_id]\n",
    "non_tool_use_samples = [preference_input[i] for i in samples_without_chosen_tool_use_id]\n",
    "\n",
    "valid_samples = tool_use_samples + non_tool_use_samples[:int(((1-tool_split)/tool_split)*len(tool_use_samples))]\n",
    "print (f\"Total valid samples for preference data: {len(valid_samples)}\")\n",
    "\n",
    "\n",
    "rejected_inputs = [s['formatted_rejected'] for s in valid_samples]\n",
    "rejected_outputs = vllm_generate(model, rejected_inputs, sampling_params,use_tqdm=True)\n",
    "for sample, output in zip(valid_samples, rejected_outputs):\n",
    "    sample['rejected'] = output\n",
    "\n",
    "successful_dpo_samples = []\n",
    "for i in range(len(valid_samples)):\n",
    "    sample = valid_samples[i]\n",
    "    input_tools = deepcopy(sample['tools'])\n",
    "    rejected_response = sample['rejected']\n",
    "    clean_response = sample['chosen']\n",
    "    ## check if rejected_response has is a tool use:\n",
    "    if '<tool_call>' in rejected_response and '</tool_call>' in rejected_response: # might have multiple tools\n",
    "        injected_tool_desc = []\n",
    "        injected_tool_match = re.findall(r\"<tool_call>\\s*(.*?)\\s*</tool_call>\", rejected_response, re.DOTALL)\n",
    "        if len(injected_tool_match):\n",
    "            for injected_tool in injected_tool_match:\n",
    "                try:\n",
    "                    injected_tool = json.loads(injected_tool)\n",
    "                    injected_tool_name = injected_tool.get('name', None)\n",
    "                    if injected_tool_name is not None:\n",
    "                        for tool in sample['rejected_tools']:\n",
    "                            if tool['function']['name'] == injected_tool_name:\n",
    "                                assert isinstance(tool,dict)\n",
    "                                injected_tool_desc.append(tool)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            if len(injected_tool_desc) > 0:\n",
    "                input_tools.extend(injected_tool_desc)\n",
    "                input_tools = [input_tools[j] for j in np.random.permutation(len(input_tools))] # shuffle the tools\n",
    "    assert isinstance(input_tools,list) and len(input_tools) > 0 and all([isinstance(t,dict) for t in input_tools]), \"Input tools should be non-empty list of dicts\"\n",
    "    # if not tool use, no need to add the additional tool\n",
    "    successful_dpo_samples.append(\n",
    "        {\n",
    "            'prompt':sample['conversations'],\n",
    "            'tools': input_tools,\n",
    "            'chosen':clean_response,\n",
    "            'rejected':rejected_response,\n",
    "        }\n",
    "    )\n",
    "\n",
    "val_size = 500 # redo the val size\n",
    "preference_data_ids = np.random.permutation(range(len(successful_dpo_samples)))\n",
    "train_dataset = [successful_dpo_samples[i] for i in range(len(successful_dpo_samples)) if i not in preference_data_ids[:val_size]]\n",
    "val_dataset = [successful_dpo_samples[i] for i in preference_data_ids[:val_size]]\n",
    "\n",
    "preference_data_path = {\n",
    "    'train':'/mnt/disk1/wjyeo/data/toucan_dpo_train.json',\n",
    "    'val':'/mnt/disk1/wjyeo/data/toucan_val_train.json', # change this later\n",
    "}\n",
    "\n",
    "jdump(train_dataset, preference_data_path['train'])\n",
    "jdump(val_dataset, preference_data_path['val'])\n",
    "train_dataset = jload(preference_data_path['train'])\n",
    "val_dataset = jload(preference_data_path['val'])\n",
    "calculate_length_for_preference_dataset(train_dataset, tokenizer)\n",
    "calculate_length_for_preference_dataset(val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73057e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
