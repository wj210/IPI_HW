{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59edd3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[32m     15\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mTOKENIZERS_PARALLELISM\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# to suppress warning message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ipi_huawei/utils/model_utils.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer,AutoModelForCausalLM\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maside\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomModelHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1406\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/__init__.py:68\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m MODULE_ATTRS:\n\u001b[32m     67\u001b[39m     module_name, attr_name = MODULE_ATTRS[name].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     module = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__package__\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/entrypoints/llm.py:18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeam_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BeamSearchInstance, BeamSearchOutput,\n\u001b[32m     16\u001b[39m                               BeamSearchSequence,\n\u001b[32m     17\u001b[39m                               create_sort_beams_key_function)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (CompilationConfig, ModelDType, TokenizerMode,\n\u001b[32m     19\u001b[39m                          is_init_field)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marg_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConvertOption, EngineArgs, HfOverrides,\n\u001b[32m     21\u001b[39m                                    PoolerConfig, RunnerOption)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/config/__init__.py:32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BlockSize, CacheConfig, CacheDType, MambaDType,\n\u001b[32m     33\u001b[39m                                PrefixCachingHashAlgo)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompilation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (CompilationConfig, CompilationLevel,\n\u001b[32m     35\u001b[39m                                      CUDAGraphMode, PassConfig)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkv_events\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KVEventsConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/config/cache.py:31\u001b[39m\n\u001b[32m     26\u001b[39m MambaDType = Literal[\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     27\u001b[39m PrefixCachingHashAlgo = Literal[\u001b[33m\"\u001b[39m\u001b[33msha256\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msha256_cbor\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m \u001b[38;5;129;43m@config\u001b[39;49m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mCacheConfig\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Configuration for the KV cache.\"\"\"\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSkipValidation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mBlockSize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/pydantic/dataclasses.py:283\u001b[39m, in \u001b[36mdataclass\u001b[39m\u001b[34m(_cls, init, repr, eq, order, unsafe_hash, frozen, config, validate_on_init, kw_only, slots)\u001b[39m\n\u001b[32m    280\u001b[39m     _pydantic_dataclasses.complete_dataclass(\u001b[38;5;28mcls\u001b[39m, config_wrapper, raise_errors=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m create_dataclass \u001b[38;5;28;01mif\u001b[39;00m _cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mcreate_dataclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/pydantic/dataclasses.py:280\u001b[39m, in \u001b[36mdataclass.<locals>.create_dataclass\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28mcls\u001b[39m.__pydantic_complete__ = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# `complete_dataclass` will set it to `True` if successful.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# TODO `parent_namespace` is currently None, but we could do the same thing as Pydantic models:\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# fetch the parent ns using `parent_frame_namespace` (if the dataclass was defined in a function),\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# and possibly cache it (see the `__pydantic_parent_namespace__` logic for models).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[43m_pydantic_dataclasses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete_dataclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py:187\u001b[39m, in \u001b[36mcomplete_dataclass\u001b[39m\u001b[34m(cls, config_wrapper, raise_errors, ns_resolver, _force_build)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# debug(schema)\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mcls\u001b[39m.__pydantic_core_schema__ = schema\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[38;5;28mcls\u001b[39m.__pydantic_validator__ = validator = \u001b[43mcreate_schema_validator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__module__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__qualname__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataclass\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplugin_settings\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28mcls\u001b[39m.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper.validate_assignment:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/pydantic/plugin/_schema_validator.py:39\u001b[39m, in \u001b[36mcreate_schema_validator\u001b[39m\u001b[34m(schema, schema_type, schema_type_module, schema_type_name, schema_kind, config, plugin_settings)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SchemaTypePath\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_plugins\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m plugins = \u001b[43mget_plugins\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m plugins:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PluggableSchemaValidator(\n\u001b[32m     42\u001b[39m         schema,\n\u001b[32m     43\u001b[39m         schema_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m         plugin_settings \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[32m     49\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/pydantic/plugin/_loader.py:40\u001b[39m, in \u001b[36mget_plugins\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m importlib_metadata.distributions():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m entry_point \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mentry_points\u001b[49m:\n\u001b[32m     41\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m entry_point.group != PYDANTIC_ENTRY_POINT_GROUP:\n\u001b[32m     42\u001b[39m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/importlib/metadata/__init__.py:471\u001b[39m, in \u001b[36mDistribution.entry_points\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mentry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints._from_text_for(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentry_points.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/importlib/metadata/__init__.py:818\u001b[39m, in \u001b[36mPathDistribution.read_text\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[32m    812\u001b[39m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[32m    813\u001b[39m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    816\u001b[39m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[32m    817\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/pathlib.py:1028\u001b[39m, in \u001b[36mPath.read_text\u001b[39m\u001b[34m(self, encoding, errors)\u001b[39m\n\u001b[32m   1024\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[33;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1027\u001b[39m encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os,json,sys\n",
    "sys.path.append(os.path.abspath(\".\")) \n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"aside\")) # to access aside.experiments \n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from utils.model_utils import load_model\n",
    "from vllm import SamplingParams\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # to suppress warning message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1bd546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 16:19:45 [utils.py:328] non-default args: {'max_model_len': 32768, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'enable_chunked_prefill': True, 'model': '/mnt/disk1/xulin/models/Qwen3-8B'}\n",
      "INFO 10-10 16:19:50 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 16:19:50 [__init__.py:1815] Using max model len 32768\n",
      "INFO 10-10 16:19:52 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 10-10 16:19:53 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 10-10 16:19:57 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:19:58 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:19:58 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/mnt/disk1/xulin/models/Qwen3-8B', speculative_config=None, tokenizer='/mnt/disk1/xulin/models/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/disk1/xulin/models/Qwen3-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1010 16:19:59.496991273 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:19:59 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m WARNING 10-10 16:19:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:00 [gpu_model_runner.py:2338] Starting to load model /mnt/disk1/xulin/models/Qwen3-8B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:00 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:00 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.27it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.44it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.82it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.59it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:03 [default_loader.py:268] Loading weights took 3.17 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:03 [gpu_model_runner.py:2392] Model loading took 15.2683 GiB and 3.362312 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:09 [backends.py:539] Using cache directory: /home/wjyeo/.cache/vllm/torch_compile_cache/426d22dfc4/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:09 [backends.py:550] Dynamo bytecode transform time: 5.34 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:10 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:13 [backends.py:215] Compiling a graph for dynamic shape takes 3.83 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:14 [monitor.py:34] torch.compile takes 9.18 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:15 [gpu_worker.py:298] Available KV cache memory: 90.83 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:15 [kv_cache_utils.py:864] GPU KV cache size: 661,424 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:15 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 20.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 23.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:19 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:19 [gpu_worker.py:391] Free memory on device (139.21/139.81 GiB) on startup. Desired GPU memory utilization is (0.8, 111.85 GiB). Actual usage is 15.27 GiB for weight, 5.68 GiB for peak activation, 0.07 GiB for non-torch memory, and 0.77 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=96544422195` to fit into requested memory, or `--kv-cache-memory=125921838592` to fully utilize gpu memory. Current kv cache memory in use is 97532180787 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1337869)\u001b[0;0m INFO 10-10 16:20:19 [core.py:218] init engine (profile, create kv cache, warmup model) took 15.37 seconds\n",
      "INFO 10-10 16:20:19 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-10 16:20:19 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"Qwen/Qwen3-8B\"\n",
    "model_path = '/mnt/disk1/xulin/models/Qwen3-8B'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "use_vllm = True\n",
    "model,tokenizer,is_aside,_ = load_model(model_path,use_vllm=use_vllm,dtype=torch.bfloat16,vllm_kwargs = {'gpu_memory_utilization':0.8,'enable_chunked_prefill':True}) # load model if generating the toucan dpo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a04b1",
   "metadata": {},
   "source": [
    "# Generate SFT for ASIDE\n",
    "\n",
    "1. Alpaca (50% without input, 50% with)\n",
    "2. Toucan (50% tool label- take 1st turn - unrotated, 50% 2nd turn onwards, rotated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a799373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "OTHER_DELM_TOKENS = {\n",
    "    'mark': ['{s}', '|{s}|', '<{s}>', '[{s}]', '<|{s}|>', '[|{s}|]', '<[{s}]>', '\\'\\'\\'{s}\\'\\'\\'', '***{s}***'],\n",
    "    'inst': ['Command', 'Rule', 'Prompt', 'Task'],\n",
    "    'inpt': ['Data', 'Context', 'Text'],\n",
    "    'resp': ['Output', 'Answer', 'Reply'],\n",
    "    'user': ['', 'Prompter ', 'User ', 'Human '],\n",
    "    'asst': ['', 'Assistant ', 'Chatbot ', 'Bot ', 'GPT ', 'AI '],\n",
    "}\n",
    "\n",
    "def create_injection_for_completion(response, instruction):\n",
    "    mark = np.random.choice(OTHER_DELM_TOKENS['mark']) + ':'\n",
    "    \n",
    "    def sample_delm(delm_name):\n",
    "        role_name = 'user' if (delm_name == 'inst' or delm_name == 'inpt') else 'asst'\n",
    "        role = np.random.choice(OTHER_DELM_TOKENS[role_name]) \n",
    "        delm = np.random.choice(OTHER_DELM_TOKENS[delm_name])\n",
    "        p = np.random.rand()\n",
    "        if p < 1/3: return (role + delm).upper()\n",
    "        elif p < 2/3: return (role + delm).lower()\n",
    "        else: return role + delm\n",
    "    \n",
    "    text = mark.format(s=sample_delm('resp')) + '\\n\\n' + response \n",
    "    text += '\\n\\n' + mark.format(s=sample_delm('inst')) + '\\n\\n' + instruction\n",
    "    return text\n",
    "\n",
    "def jload(f, mode=\"r\", num_samples=None):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    if isinstance(obj, (dict, list)): json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str): f.write(obj)\n",
    "    else: raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def calculate_length_for_preference_dataset(dataset, tokenizer):\n",
    "    chosen_input_ids = tokenizer([d['chosen'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "    rejected_input_ids = tokenizer([d['rejected'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    chosen_lengths = np.array([len(prompt) for prompt in chosen_input_ids])\n",
    "    rejected_lengths = np.array([len(prompt) for prompt in rejected_input_ids])\n",
    "    prompt_and_label_lengths = np.maximum(chosen_lengths,rejected_lengths)\n",
    "\n",
    "    print('Input+Output model_max_length (98%, 99%, 99.5%, 99.9%):', np.percentile(prompt_and_label_lengths, [95, 99, 99.5, 99.9]))\n",
    "    print (f'Mean: {(np.mean(chosen_lengths) + np.mean(rejected_lengths))/2:.2f} Num > 2048: {np.sum(prompt_and_label_lengths>2048)} / {len(prompt_and_label_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff41d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51760\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/mnt/disk1/wjyeo/data\"\n",
    "sft_data_path = {'train':os.path.join(data_dir, \"alpaca_aside_train.json\"),\n",
    "                        'val':os.path.join(data_dir, \"alpaca_aside_val.json\"),}\n",
    "clean_alpaca = load_dataset(\"yahma/alpaca-cleaned\")['train']\n",
    "print (len(clean_alpaca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8db22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157 32603\n",
      "38314\n"
     ]
    }
   ],
   "source": [
    "alpaca_sampling_params = SamplingParams(temperature=0., max_tokens=2048, stop=tokenizer.eos_token)\n",
    "\n",
    "data_with_input = []\n",
    "data_without_input = []\n",
    "for i,sample in enumerate(clean_alpaca):\n",
    "    if sample.get('input', '').strip() == '':\n",
    "        data_without_input.append(sample)\n",
    "    else:\n",
    "        data_with_input.append(sample)\n",
    "print (len(data_with_input), len(data_without_input))\n",
    "\n",
    "balanced_alpaca = []\n",
    "num_samples = min(len(data_with_input), len(data_without_input))\n",
    "if len(data_with_input) > num_samples:\n",
    "    data_with_input = np.random.choice(data_with_input, num_samples, replace=False).tolist()\n",
    "if len(data_without_input) > num_samples:\n",
    "    data_without_input = np.random.choice(data_without_input, num_samples, replace=False).tolist()\n",
    "balanced_alpaca = data_with_input + data_without_input\n",
    "print (len(balanced_alpaca))\n",
    "np.random.shuffle(balanced_alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_inputs = []\n",
    "for sample in balanced_alpaca:\n",
    "    if sample.get('input','').strip() != '':\n",
    "        conv = [\n",
    "            {\"role\": \"user\", \"content\": sample[\"instruction\"] + \"\\n\\n\" + sample[\"input\"]}\n",
    "        ]\n",
    "    else:\n",
    "        conv = [\n",
    "            {\"role\": \"user\", \"content\": sample[\"instruction\"]}\n",
    "        ]\n",
    "    sft_inputs.append(tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "print (f'SFT dataset size: {len(sft_inputs)}')  \n",
    "sft_outputs = model.generate(sft_inputs, alpaca_sampling_params,use_tqdm=True)\n",
    "sft_outputs = [o.outputs[0].text for o in sft_outputs]\n",
    "\n",
    "sft_data = []\n",
    "for sample,output in zip(balanced_alpaca,sft_outputs):\n",
    "    if sample.get('input','').strip() == '':\n",
    "        sft_data.append({\n",
    "            'instruction': sample['instruction'],\n",
    "            'output': output\n",
    "        })\n",
    "    else:\n",
    "        sft_data.append({\n",
    "            'instruction': sample['instruction'],\n",
    "            'input': sample.get('input',''),\n",
    "            'output': output\n",
    "        })\n",
    "\n",
    "val_size= 500\n",
    "random_ids = np.random.permutation(len(sft_data))\n",
    "sft_data_train = [sft_data[i] for i in random_ids[val_size:]]\n",
    "sft_data_val = [sft_data[i] for i in random_ids[:val_size]]\n",
    "print (f'SFT train size: {len(sft_data_train)}, val size: {len(sft_data_val)}')\n",
    "\n",
    "jdump(sft_data_train,sft_data_path['train'])\n",
    "jdump(sft_data_val,sft_data_path['val'])\n",
    "\n",
    "## Get length\n",
    "input_len = [len(ids) for ids in tokenizer([d['instruction'] + '\\n\\n' + d.get('input','') for d in sft_data_train])[\"input_ids\"]]\n",
    "completion_len = [len(ids) for ids in tokenizer([d['output'] for d in sft_data_train])[\"input_ids\"]]\n",
    "overall_len = [i+c for i,c in zip(input_len,completion_len)]\n",
    "print (f'Mean/Max/Min/>2048: {np.mean(overall_len):.2f} / {np.max(overall_len)} / {np.min(overall_len)} / {np.sum(np.array(overall_len)>2048)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7c6fa",
   "metadata": {},
   "source": [
    "# Generate onpolicy for Toucan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5691b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9ed9751e2a4e0989ed5f243ea4f3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('Agent-Ark/Toucan-1.5M','SFT',split = 'train').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f108c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_json_loads(value): # THE JSON tool call labels are messy.\n",
    "    \"\"\"Try to parse value as JSON, otherwise return as-is.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed = json.loads(value)\n",
    "            # Recursively process the parsed result as well\n",
    "            return recursively_fix(parsed)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "def recursively_fix(obj):\n",
    "    \"\"\"Recursively decode nested JSON strings inside dicts/lists.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursively_fix(try_json_loads(v)) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursively_fix(try_json_loads(x)) for x in obj]\n",
    "    else:\n",
    "        return try_json_loads(obj)\n",
    "\n",
    "def fix_tool_response(tool_response):\n",
    "    tool_response = ast.literal_eval(tool_response)\n",
    "    tool_response = recursively_fix(tool_response)\n",
    "    return json.dumps(tool_response, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82779230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119287/119287 [00:25<00:00, 4757.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid cat no_tool_use: 40000 samples\n",
      "Valid cat rotated: 259452 samples\n",
      "Valid cat non-rotated: 79340 samples\n",
      "Total samples with assistant tool calls: 0 / 119287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## reformat the dataset\n",
    "toolcall_format = \"<tool_call>\\n{tool}\\n</tool_call>\\n\"\n",
    "\n",
    "# Get the length.\n",
    "def get_len(sample):\n",
    "    convs = sample['conversations']\n",
    "    formatted = tool_prompt_format(convs,sample['tools'],tokenizer)\n",
    "    sample['len'] =len(tokenizer.encode(formatted))\n",
    "    return sample\n",
    "\n",
    "cat_ds = defaultdict(list)\n",
    "invalid_cats = defaultdict(int)\n",
    "\n",
    "for sample in tqdm(ds,total = len(ds)):\n",
    "    try: # have to be able to parse both of them.\n",
    "        tools = json.loads(sample['tools'])\n",
    "        messages = json.loads(sample['messages'])\n",
    "    except:\n",
    "        invalid_cats['parsing_error'] += 1\n",
    "        continue\n",
    "    \n",
    "    cat = sample['subset_name']\n",
    "    roles = [m['role'] for m in messages]\n",
    "    if not any([r == 'tool_response' for r in roles]): # no tool roles\n",
    "        invalid_cats['no_tool_use'] += 1\n",
    "        continue\n",
    "    \n",
    "    structured_msg = []\n",
    "    curr_tool_call = ''\n",
    "    assistant_start = False\n",
    "    turns_with_response = [] # We add valid assistant turns with the option of choosing any random turn to only do SFT on that turn, the issue is that in Qwen, the no-thinking mode causes extra tokens <think> and </think> to be added only to the last turn, thus we if train on all turns, usually only the tool turns are earlier and do not have these <think> tokens, thus there will be a mismatch.\n",
    "    tool_started = False # used to track if the conversation includes a tool response.\n",
    "    turns_without_response = []\n",
    "\n",
    "\n",
    "    for msg_id,m in enumerate(messages): # we ignore the assistant role since Qwen do not generate any text other than the MCP tools\n",
    "        if m['role'] == 'tool_call':\n",
    "            tool_response = fix_tool_response(m['content'])\n",
    "            curr_tool_call += toolcall_format.format(tool = tool_response)\n",
    "        else:\n",
    "            if curr_tool_call != '': # if the next msg is not tool_call, we dump the curr_tool_call\n",
    "                structured_msg.append({'role':'assistant','content':curr_tool_call.strip()})\n",
    "                curr_tool_call = ''\n",
    "                if tool_started:\n",
    "                    turns_with_response.append(len(structured_msg)-1)\n",
    "                else: # keep track until which turn it doesnt have any response.\n",
    "                    turns_without_response.append(len(structured_msg)-1)\n",
    "\n",
    "            if m['role'] == 'user':\n",
    "                structured_msg.append(m)\n",
    "            elif m['role'] == 'tool_response':\n",
    "                tool_started = True\n",
    "                structured_msg.append({'role':'tool','content':m['content']})\n",
    "            elif m['role'] == 'assistant': # if is assistant, we only keep it if it is the last msg or if the next msg is not tool_call.\n",
    "                if msg_id == len(messages) - 1 or messages[msg_id + 1]['role'] != 'tool_call':\n",
    "                    structured_msg.append(m)\n",
    "                    if tool_started:\n",
    "                        turns_with_response.append(len(structured_msg)-1)\n",
    "                    else:\n",
    "                        turns_without_response.append(len(structured_msg)-1)\n",
    "\n",
    "    for turn in turns_with_response:\n",
    "        truncated_msg = structured_msg[:turn]\n",
    "        assert truncated_msg[-1]['role'] != 'assistant', \"Last message should not be assistant!\"\n",
    "        cat_ds['rotated'].append(\n",
    "        {'conversations':truncated_msg,\n",
    "         'tools':tools,})\n",
    "    for turn in turns_without_response:\n",
    "        truncated_msg = structured_msg[:turn]\n",
    "        assert truncated_msg[-1]['role'] != 'assistant', \"Last message should not be assistant!\"\n",
    "        cat_ds['non-rotated'].append(\n",
    "        {'conversations':truncated_msg,\n",
    "         'tools':tools,})\n",
    "    # if np.random.rand() < 0.5: \n",
    "    #     chosen_turn = np.random.choice(turns_with_response)\n",
    "    #     cat_label = 'rotated'\n",
    "    # else:\n",
    "    #     chosen_turn = np.random.choice(turns_without_response)\n",
    "    #     cat_label = 'non-rotated'\n",
    "    # structured_msg = structured_msg[:chosen_turn]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "for k in invalid_cats:\n",
    "    print(f\"Invalid cat {k}: {invalid_cats[k]} samples\")\n",
    "\n",
    "for k in cat_ds:\n",
    "    print(f\"Valid cat {k}: {len(cat_ds[k])} samples\")\n",
    "\n",
    "print (f\"Total samples with assistant tool calls: {len(cat_ds['tool'])} / {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66dd0103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (95538 > 32768). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (95538 > 32768). Running this sequence through the model will result in indexing errors\n",
      "Computing input lengths: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 259452/259452 [04:50<00:00, 893.64it/s]\n",
      "Computing input lengths: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79340/79340 [01:06<00:00, 1199.94it/s]\n",
      "Computing input lengths: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use multi process to get the lengths\n",
    "cat_ds_w_len = {k:async_process(get_len,cat_ds[k],workers=32, msg=\"Computing input lengths\") for k in cat_ds}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6fe075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat rotated: before filtering 259452, after filtering 137836\n",
      "Cat non-rotated: before filtering 79340, after filtering 54100\n",
      "Cat tool: before filtering 0, after filtering 0\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 4096-512\n",
    "cat_ds_valid = {k:[s for s in cat_ds_w_len[k] if s['len'] <= MAX_LENGTH] for k in cat_ds_w_len}\n",
    "for k in cat_ds_w_len:\n",
    "    print (f\"Cat {k}: before filtering {len(cat_ds_w_len[k])}, after filtering {len(cat_ds_valid[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4497f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7d3acd26b34306a7735513a2520f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/191936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3101571fe474322b21f83e1d38f70b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we should balance it out, since the dataset has more non-tool samples. we take an equal amount of tool and non-tool samples.\n",
    "tool_sampling_params = SamplingParams(temperature=0., max_tokens=512)\n",
    "rotated_ds = cat_ds_valid['rotated']\n",
    "non_rotated_ds = cat_ds_valid['non-rotated']\n",
    "\n",
    "balanced_ds = rotated_ds + non_rotated_ds\n",
    "np.random.shuffle(balanced_ds)\n",
    "\n",
    "balanced_inputs = [tool_prompt_format(m['conversations'],m['tools'],tokenizer) for m in balanced_ds]\n",
    "\n",
    "balanced_outputs = model.generate(balanced_inputs, tool_sampling_params,use_tqdm=True)\n",
    "balanced_outputs = [o.outputs[0].text for o in balanced_outputs]\n",
    "\n",
    "\n",
    "for sample,output in zip(balanced_ds,balanced_outputs):\n",
    "    sample['conversations'].append({'role':'assistant','content':output})\n",
    "\n",
    "random_ids = np.random.permutation(len(balanced_ds))\n",
    "val_size = 1000\n",
    "train_ids = random_ids[val_size:]\n",
    "val_ids = random_ids[:val_size]\n",
    "\n",
    "train_ds = [balanced_ds[i] for i in train_ids]\n",
    "val_ds = [balanced_ds[i] for i in val_ids]\n",
    "\n",
    "print (f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")\n",
    "\n",
    "with open('/mnt/disk1/wjyeo/data/toucan_aside_train.json','w') as f:\n",
    "    json.dump(train_ds,f,indent = 4)\n",
    "with open('/mnt/disk1/wjyeo/data/toucan_aside_val.json','w') as f:\n",
    "    json.dump(val_ds,f,indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
