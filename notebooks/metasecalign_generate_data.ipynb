{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d70e38",
   "metadata": {},
   "source": [
    "# MetaSecAlign\n",
    "\n",
    "Generate on-policy completion for Alpaca. Code adapted from https://github.com/facebookresearch/Meta_SecAlign/blob/main/generate_data.py to use vllm for fast inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6ac6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,sys,json\n",
    "import torch\n",
    "sys.path.append(os.path.abspath(\".\")) \n",
    "sys.path.append(os.path.abspath(\"aside\"))\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import io\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy   \n",
    "from constants import *\n",
    "from utils.model_utils import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec54862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jload(f, mode=\"r\", num_samples=None):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    if num_samples is not None and num_samples > 0 and num_samples < len(jdict):\n",
    "        random.seed(10)\n",
    "        jdict = random.sample(jdict, num_samples)\n",
    "        random.seed(time.time())\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    if isinstance(obj, (dict, list)): json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str): f.write(obj)\n",
    "    else: raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "OTHER_DELM_TOKENS = {\n",
    "    'mark': ['{s}', '|{s}|', '<{s}>', '[{s}]', '<|{s}|>', '[|{s}|]', '<[{s}]>', '\\'\\'\\'{s}\\'\\'\\'', '***{s}***'],\n",
    "    'inst': ['Command', 'Rule', 'Prompt', 'Task'],\n",
    "    'inpt': ['Data', 'Context', 'Text'],\n",
    "    'resp': ['Output', 'Answer', 'Reply'],\n",
    "    'user': ['', 'Prompter ', 'User ', 'Human '],\n",
    "    'asst': ['', 'Assistant ', 'Chatbot ', 'Bot ', 'GPT ', 'AI '],\n",
    "}\n",
    "\n",
    "def create_injection_for_completion(response, instruction, input):\n",
    "    mark = np.random.choice(OTHER_DELM_TOKENS['mark']) + ':'\n",
    "    \n",
    "    def sample_delm(delm_name):\n",
    "        role_name = 'user' if (delm_name == 'inst' or delm_name == 'inpt') else 'asst'\n",
    "        role = np.random.choice(OTHER_DELM_TOKENS[role_name]) \n",
    "        delm = np.random.choice(OTHER_DELM_TOKENS[delm_name])\n",
    "        p = np.random.rand()\n",
    "        if p < 1/3: return (role + delm).upper()\n",
    "        elif p < 2/3: return (role + delm).lower()\n",
    "        else: return role + delm\n",
    "    \n",
    "    text = mark.format(s=sample_delm('resp')) + '\\n\\n' + response \n",
    "    text += '\\n\\n' + mark.format(s=sample_delm('inst')) + '\\n\\n' + instruction\n",
    "    if input != '':  text += '\\n\\n' + mark.format(s=sample_delm('inpt')) + '\\n\\n' + input\n",
    "    return text\n",
    "\n",
    "def calculate_length_for_preference_dataset(dataset, tokenizer):\n",
    "    chosen_input_ids = tokenizer([d['chosen'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "    rejected_input_ids = tokenizer([d['rejected'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    chosen_lengths = np.array([len(prompt) for prompt in chosen_input_ids])\n",
    "    rejected_lengths = np.array([len(prompt) for prompt in rejected_input_ids])\n",
    "    prompt_and_label_lengths = np.maximum(chosen_lengths,rejected_lengths)\n",
    "\n",
    "    print('Input+Output model_max_length (98%, 99%, 99.5%, 99.9%):', np.percentile(prompt_and_label_lengths, [95, 99, 99.5, 99.9]))\n",
    "    print (f'Mean: {(np.mean(chosen_lengths) + np.mean(rejected_lengths))/2:.2f} Num > 2048: {np.sum(prompt_and_label_lengths>2048)} / {len(prompt_and_label_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8846b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51760\n"
     ]
    }
   ],
   "source": [
    "clean_data = load_dataset(\"yahma/alpaca-cleaned\")['train']\n",
    "print (len(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure the ctx len is loaded, default is 32768\n",
      "INFO 10-14 15:12:49 [utils.py:328] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 4, 'disable_log_stats': True, 'enable_chunked_prefill': False, 'model': 'meta-llama/Llama-3.1-8B-Instruct'}\n"
     ]
    }
   ],
   "source": [
    "model_path = LLAMA_PATH # llama or qwen\n",
    "model,tokenizer,_,_ = load_model(model_path,use_vllm=True)\n",
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=2048, stop=tokenizer.eos_token)\n",
    "\n",
    "\n",
    "# preference_data_path = {'train':os.path.join(DATA_DIR, f\"{model.m_name}_metasecalign_train.json\"),\n",
    "#                         'val':os.path.join(DATA_DIR, f\"{model.m_name}_metasecalign_val.json\"),}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b9a26",
   "metadata": {},
   "source": [
    "# Adversarial Training\n",
    "\n",
    "Only take samples with data input, since DPO requires the rejected to be the attack's response. Data without input cannot have IPI applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d4dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157\n"
     ]
    }
   ],
   "source": [
    "data_with_injection = []\n",
    "for i,sample in enumerate(clean_data):\n",
    "    if sample.get('input', '').strip() == '':\n",
    "        continue\n",
    "    data_with_injection.append(sample)\n",
    "print (len(data_with_injection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db23f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 19157/19157 [00:22<00:00, 841.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preference_data = []\n",
    "for i,sample in tqdm(enumerate(data_with_injection),total = len(data_with_injection)):\n",
    "    current_sample = deepcopy(sample)\n",
    "    injected_sample = deepcopy(sample)\n",
    "    instruction = current_sample['instruction']\n",
    "    inpt = current_sample['input']\n",
    "\n",
    "    while injected_sample['instruction'] == current_sample['instruction']:\n",
    "        injected_sample = np.random.choice(data_with_injection, size=1, replace=False)[0]\n",
    "    \n",
    "    injected_prompt = injected_sample['instruction'] + ' ' + injected_sample['input']\n",
    "    if np.random.rand() < 0.9:  # 90% Straightforward Attack, 10% Completion Attack\n",
    "        current_sample['input'] = injected_prompt + ' ' + current_sample['input'] if np.random.rand() < 0.5 else current_sample['input'] + ' ' + injected_prompt\n",
    "    else: \n",
    "        fake_response = current_sample['output']\n",
    "        current_sample['input'] += '\\n\\n' + create_injection_for_completion(fake_response, injected_sample['instruction'], injected_sample['input'])\n",
    "    preference_data.append({\n",
    "                'instruction': current_sample['instruction'],\n",
    "                'input': current_sample['input'],\n",
    "                'chosen_input': instruction + '\\n\\n' + inpt,\n",
    "                'rejected_input': injected_sample['instruction'] + ' ' + injected_sample['input'],\n",
    "            })\n",
    "print (len(preference_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51bcece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b831f86ccd64bac8d05ce578217f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/38314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6401e151bea842eba958cd63df97b2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                  | 0/38314 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     conversations.append(tokenizer.apply_chat_template([{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: sample[\u001b[33m\"\u001b[39m\u001b[33mchosen_input\u001b[39m\u001b[33m\"\u001b[39m]}], tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,enable_thinking=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m      5\u001b[39m     conversations.append(tokenizer.apply_chat_template([{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: sample[\u001b[33m\"\u001b[39m\u001b[33mrejected_input\u001b[39m\u001b[33m\"\u001b[39m]}], tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,enable_thinking=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(preference_data)):\n\u001b[32m      8\u001b[39m     sample = preference_data[i]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/entrypoints/llm.py:396\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    385\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    386\u001b[39m     prompts, lora_request)\n\u001b[32m    388\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    389\u001b[39m     prompts=prompts,\n\u001b[32m    390\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    393\u001b[39m     priority=priority,\n\u001b[32m    394\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/entrypoints/llm.py:1550\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1548\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1552\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:248\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    251\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:668\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EngineCoreOutputs:\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[32m    667\u001b[39m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutputs_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m    670\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/queue.py:171\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a non-negative number\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ipi_hw/lib/python3.12/threading.py:334\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=2048, stop=tokenizer.eos_token)\n",
    "conversations = []\n",
    "for sample in preference_data:\n",
    "    conversations.append(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sample[\"chosen_input\"]}], tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "    conversations.append(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sample[\"rejected_input\"]}], tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "outputs = model.generate(conversations, sampling_params,use_tqdm=True)\n",
    "for i in range(len(preference_data)):\n",
    "    sample = preference_data[i]\n",
    "    sample['chosen'] = outputs[2*i].outputs[0].text + tokenizer.eos_token\n",
    "    sample['rejected'] = outputs[2*i+1].outputs[0].text + tokenizer.eos_token\n",
    "\n",
    "val_size = 500\n",
    "preference_data_ids = np.random.permutation(range(len(preference_data)))\n",
    "train_dataset = [preference_data[i] for i in range(len(preference_data)) if i not in preference_data_ids[:val_size]]\n",
    "val_dataset = [preference_data[i] for i in preference_data_ids[:val_size]]\n",
    "\n",
    "jdump(train_dataset, preference_data_path['train'])\n",
    "jdump(val_dataset, preference_data_path['val'])\n",
    "train_dataset = load_dataset('json', data_files=preference_data_path['train'], split='train')\n",
    "val_dataset = load_dataset('json', data_files=preference_data_path['val'], split='train')\n",
    "calculate_length_for_preference_dataset(train_dataset, tokenizer)\n",
    "calculate_length_for_preference_dataset(val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a74e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45e31d10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
