{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6ac6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 15:55:23 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os,sys,json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import io\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec54862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jload(f, mode=\"r\", num_samples=None):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    if num_samples is not None and num_samples > 0 and num_samples < len(jdict):\n",
    "        random.seed(10)\n",
    "        jdict = random.sample(jdict, num_samples)\n",
    "        random.seed(time.time())\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    if isinstance(obj, (dict, list)): json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str): f.write(obj)\n",
    "    else: raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "OTHER_DELM_TOKENS = {\n",
    "    'mark': ['{s}', '|{s}|', '<{s}>', '[{s}]', '<|{s}|>', '[|{s}|]', '<[{s}]>', '\\'\\'\\'{s}\\'\\'\\'', '***{s}***'],\n",
    "    'inst': ['Command', 'Rule', 'Prompt', 'Task'],\n",
    "    'inpt': ['Data', 'Context', 'Text'],\n",
    "    'resp': ['Output', 'Answer', 'Reply'],\n",
    "    'user': ['', 'Prompter ', 'User ', 'Human '],\n",
    "    'asst': ['', 'Assistant ', 'Chatbot ', 'Bot ', 'GPT ', 'AI '],\n",
    "}\n",
    "\n",
    "def create_injection_for_completion(response, instruction, input):\n",
    "    mark = np.random.choice(OTHER_DELM_TOKENS['mark']) + ':'\n",
    "    \n",
    "    def sample_delm(delm_name):\n",
    "        role_name = 'user' if (delm_name == 'inst' or delm_name == 'inpt') else 'asst'\n",
    "        role = np.random.choice(OTHER_DELM_TOKENS[role_name]) \n",
    "        delm = np.random.choice(OTHER_DELM_TOKENS[delm_name])\n",
    "        p = np.random.rand()\n",
    "        if p < 1/3: return (role + delm).upper()\n",
    "        elif p < 2/3: return (role + delm).lower()\n",
    "        else: return role + delm\n",
    "    \n",
    "    text = mark.format(s=sample_delm('resp')) + '\\n\\n' + response \n",
    "    text += '\\n\\n' + mark.format(s=sample_delm('inst')) + '\\n\\n' + instruction\n",
    "    if input != '':  text += '\\n\\n' + mark.format(s=sample_delm('inpt')) + '\\n\\n' + input\n",
    "    return text\n",
    "\n",
    "def calculate_length_for_preference_dataset(dataset, tokenizer):\n",
    "    chosen_input_ids = tokenizer([d['chosen'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "    rejected_input_ids = tokenizer([d['rejected'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    chosen_lengths = np.array([len(prompt) for prompt in chosen_input_ids])\n",
    "    rejected_lengths = np.array([len(prompt) for prompt in rejected_input_ids])\n",
    "    prompt_and_label_lengths = np.maximum(chosen_lengths,rejected_lengths)\n",
    "\n",
    "    print('Input+Output model_max_length (98%, 99%, 99.5%, 99.9%):', np.percentile(prompt_and_label_lengths, [95, 99, 99.5, 99.9]))\n",
    "    print (f'Mean: {(np.mean(chosen_lengths) + np.mean(rejected_lengths))/2:.2f} Num > 2048: {np.sum(prompt_and_label_lengths>2048)} / {len(prompt_and_label_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8846b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51760\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/mnt/disk1/wjyeo/data\"\n",
    "preference_data_path = {'train':os.path.join(data_dir, \"metasecalign_train.json\"),\n",
    "                        'val':os.path.join(data_dir, \"metasecalign_val.json\"),}\n",
    "\n",
    "clean_data = load_dataset(\"yahma/alpaca-cleaned\")['train']\n",
    "print (len(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 15:23:48 [utils.py:328] non-default args: {'trust_remote_code': True, 'disable_log_stats': True, 'model': '/mnt/disk1/xulin/models/Qwen3-8B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 15:23:53 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 15:23:53 [__init__.py:1815] Using max model len 40960\n",
      "INFO 10-06 15:23:55 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:23:56 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:23:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/mnt/disk1/xulin/models/Qwen3-8B', speculative_config=None, tokenizer='/mnt/disk1/xulin/models/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/disk1/xulin/models/Qwen3-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:23:58 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m WARNING 10-06 15:23:58 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:23:58 [gpu_model_runner.py:2338] Starting to load model /mnt/disk1/xulin/models/Qwen3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1006 15:23:58.083739371 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:23:58 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:23:58 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1913955dda249a1ac4c907e041a5593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:02 [default_loader.py:268] Loading weights took 3.24 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:02 [gpu_model_runner.py:2392] Model loading took 15.2683 GiB and 3.423974 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:08 [backends.py:539] Using cache directory: /home/wjyeo/.cache/vllm/torch_compile_cache/00dcc66913/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:08 [backends.py:550] Dynamo bytecode transform time: 5.31 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.655 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:12 [monitor.py:34] torch.compile takes 5.31 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:13 [gpu_worker.py:298] Available KV cache memory: 104.82 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:13 [kv_cache_utils.py:864] GPU KV cache size: 763,232 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:13 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 18.63x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:02<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:16 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:16 [gpu_worker.py:391] Free memory on device (139.21/139.81 GiB) on startup. Desired GPU memory utilization is (0.9, 125.83 GiB). Actual usage is 15.27 GiB for weight, 5.68 GiB for peak activation, 0.07 GiB for non-torch memory, and 0.77 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=111556550553` to fit into requested memory, or `--kv-cache-memory=125921838592` to fully utilize gpu memory. Current kv cache memory in use is 112544309145 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=571756)\u001b[0;0m INFO 10-06 15:24:16 [core.py:218] init engine (profile, create kv cache, warmup model) took 13.99 seconds\n",
      "INFO 10-06 15:24:17 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-06 15:24:17 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 10-06 15:41:32 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "model_path = '/mnt/disk1/xulin/models/Qwen3-8B'\n",
    "model = LLM(model_path, tensor_parallel_size=torch.cuda.device_count(), trust_remote_code=True) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=2048, stop=tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680927f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0., max_tokens=2048, stop=tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b9a26",
   "metadata": {},
   "source": [
    "# ASIDE/ISE\n",
    "\n",
    "For approaches with learnt embeddings, we first get the SFT dataset, by just getting on-policy rollouts on the full alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d4dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157\n"
     ]
    }
   ],
   "source": [
    "data_with_injection = []\n",
    "for i,sample in enumerate(clean_data):\n",
    "    if sample.get('input', '').strip() == '':\n",
    "        continue\n",
    "    data_with_injection.append(sample)\n",
    "print (len(data_with_injection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc3dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT dataset size: 19157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dd8ccf72ff4c48a3fae43b479b7e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/19157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdef191c2aa34e5b898620f709fc93d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT train size: 18657, val size: 500\n",
      "Mean/Max/Min/>2048: 248.08 / 2468 / 7 / 51\n"
     ]
    }
   ],
   "source": [
    "sft_inputs = []\n",
    "for sample in data_with_injection:\n",
    "    conv = [\n",
    "        {\"role\": \"user\", \"content\": sample[\"instruction\"] + \"\\n\\n\" + sample[\"input\"]}\n",
    "    ]\n",
    "    sft_inputs.append(tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "print (f'SFT dataset size: {len(sft_inputs)}')  \n",
    "sft_outputs = model.generate(sft_inputs, sampling_params,use_tqdm=True)\n",
    "sft_outputs = [o.outputs[0].text for o in sft_outputs]\n",
    "\n",
    "sft_data = []\n",
    "for sample,output in zip(data_with_injection,sft_outputs):\n",
    "    sft_data.append({\n",
    "        'instruction': sample['instruction'],\n",
    "        'input': sample['input'],\n",
    "        'output': output\n",
    "    })\n",
    "    \n",
    "val_size= 500\n",
    "random_ids = np.random.permutation(len(sft_data))\n",
    "sft_data_train = [sft_data[i] for i in random_ids[val_size:]]\n",
    "sft_data_val = [sft_data[i] for i in random_ids[:val_size]]\n",
    "print (f'SFT train size: {len(sft_data_train)}, val size: {len(sft_data_val)}')\n",
    "\n",
    "sft_data_path = {'train':os.path.join(data_dir, \"alpaca_rollout_train.json\"),\n",
    "                        'val':os.path.join(data_dir, \"alpaca_rollout_val.json\"),}\n",
    "\n",
    "jdump(sft_data_train,sft_data_path['train'])\n",
    "jdump(sft_data_val,sft_data_path['val'])\n",
    "\n",
    "## Get length\n",
    "input_len = [len(ids) for ids in tokenizer([d['instruction'] + '\\n\\n' + d['input'] for d in sft_data_train])[\"input_ids\"]]\n",
    "completion_len = [len(ids) for ids in tokenizer([d['response'] for d in sft_data_train])[\"input_ids\"]]\n",
    "overall_len = [i+c for i,c in zip(input_len,completion_len)]\n",
    "print (f'Mean/Max/Min/>2048: {np.mean(overall_len):.2f} / {np.max(overall_len)} / {np.min(overall_len)} / {np.sum(np.array(overall_len)>2048)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19157/19157 [00:24<00:00, 784.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preference_data = []\n",
    "for i,sample in tqdm(enumerate(data_with_injection),total = len(data_with_injection)):\n",
    "    current_sample = deepcopy(sample)\n",
    "    injected_sample = deepcopy(sample)\n",
    "    instruction = current_sample['instruction']\n",
    "    inpt = current_sample['input']\n",
    "\n",
    "    while injected_sample['instruction'] == current_sample['instruction']:\n",
    "        injected_sample = np.random.choice(data_with_injection, size=1, replace=False)[0]\n",
    "    \n",
    "    injected_prompt = injected_sample['instruction'] + ' ' + injected_sample['input']\n",
    "    if np.random.rand() < 0.9:  # 90% Straightforward Attack, 10% Completion Attack\n",
    "        current_sample['input'] = injected_prompt + ' ' + current_sample['input'] if np.random.rand() < 0.5 else current_sample['input'] + ' ' + injected_prompt\n",
    "    else: \n",
    "        fake_response = current_sample['output']\n",
    "        current_sample['input'] += '\\n\\n' + create_injection_for_completion(fake_response, injected_sample['instruction'], injected_sample['input'])\n",
    "    preference_data.append({\n",
    "                'instruction': current_sample['instruction'],\n",
    "                'input': current_sample['input'],\n",
    "                'chosen_input': instruction + '\\n\\n' + inpt,\n",
    "                'rejected_input': injected_sample['instruction'] + ' ' + injected_sample['input'],\n",
    "            })\n",
    "print (len(preference_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bcece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=2048, stop=tokenizer.eos_token)\n",
    "conversations = []\n",
    "for sample in preference_data:\n",
    "    conversations.append(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sample[\"chosen_input\"]}], tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "    conversations.append(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sample[\"rejected_input\"]}], tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "outputs = model.generate(conversations, sampling_params,use_tqdm=True)\n",
    "for i in range(len(preference_data)):\n",
    "    sample = preference_data[i]\n",
    "    sample['chosen'] = outputs[2*i].outputs[0].text + tokenizer.eos_token\n",
    "    sample['rejected'] = outputs[2*i+1].outputs[0].text + tokenizer.eos_token\n",
    "\n",
    "val_size = 500\n",
    "preference_data_ids = np.random.permutation(range(len(preference_data)))\n",
    "train_dataset = [preference_data[i] for i in range(len(preference_data)) if i not in preference_data_ids[:val_size]]\n",
    "val_dataset = [preference_data[i] for i in preference_data_ids[:val_size]]\n",
    "\n",
    "jdump(train_dataset, preference_data_path['train'])\n",
    "jdump(val_dataset, preference_data_path['val'])\n",
    "train_dataset = load_dataset('json', data_files=preference_data_path['train'], split='train')\n",
    "val_dataset = load_dataset('json', data_files=preference_data_path['val'], split='train')\n",
    "calculate_length_for_preference_dataset(train_dataset, tokenizer)\n",
    "calculate_length_for_preference_dataset(val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a74e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipi_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
