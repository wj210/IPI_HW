{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d70e38",
   "metadata": {},
   "source": [
    "# MetaSecAlign\n",
    "\n",
    "Generate on-policy completion for Alpaca. Code adapted from https://github.com/facebookresearch/Meta_SecAlign/blob/main/generate_data.py to use vllm for fast inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6ac6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,sys,json\n",
    "import torch\n",
    "sys.path.append(os.path.abspath(\".\")) \n",
    "sys.path.append(os.path.abspath(\"aside\"))\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import io\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy   \n",
    "from constants import *\n",
    "from utils.model_utils import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec54862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jload(f, mode=\"r\", num_samples=None):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    if num_samples is not None and num_samples > 0 and num_samples < len(jdict):\n",
    "        random.seed(10)\n",
    "        jdict = random.sample(jdict, num_samples)\n",
    "        random.seed(time.time())\n",
    "    return jdict\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    if not isinstance(f, io.IOBase): f = open(f, mode=mode)\n",
    "    if isinstance(obj, (dict, list)): json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str): f.write(obj)\n",
    "    else: raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "OTHER_DELM_TOKENS = {\n",
    "    'mark': ['{s}', '|{s}|', '<{s}>', '[{s}]', '<|{s}|>', '[|{s}|]', '<[{s}]>', '\\'\\'\\'{s}\\'\\'\\'', '***{s}***'],\n",
    "    'inst': ['Command', 'Rule', 'Prompt', 'Task'],\n",
    "    'inpt': ['Data', 'Context', 'Text'],\n",
    "    'resp': ['Output', 'Answer', 'Reply'],\n",
    "    'user': ['', 'Prompter ', 'User ', 'Human '],\n",
    "    'asst': ['', 'Assistant ', 'Chatbot ', 'Bot ', 'GPT ', 'AI '],\n",
    "}\n",
    "\n",
    "def create_injection_for_completion(response, instruction, input):\n",
    "    mark = np.random.choice(OTHER_DELM_TOKENS['mark']) + ':'\n",
    "    \n",
    "    def sample_delm(delm_name):\n",
    "        role_name = 'user' if (delm_name == 'inst' or delm_name == 'inpt') else 'asst'\n",
    "        role = np.random.choice(OTHER_DELM_TOKENS[role_name]) \n",
    "        delm = np.random.choice(OTHER_DELM_TOKENS[delm_name])\n",
    "        p = np.random.rand()\n",
    "        if p < 1/3: return (role + delm).upper()\n",
    "        elif p < 2/3: return (role + delm).lower()\n",
    "        else: return role + delm\n",
    "    \n",
    "    text = mark.format(s=sample_delm('resp')) + '\\n\\n' + response \n",
    "    text += '\\n\\n' + mark.format(s=sample_delm('inst')) + '\\n\\n' + instruction\n",
    "    if input != '':  text += '\\n\\n' + mark.format(s=sample_delm('inpt')) + '\\n\\n' + input\n",
    "    return text\n",
    "\n",
    "def calculate_length_for_preference_dataset(dataset, tokenizer):\n",
    "    chosen_input_ids = tokenizer([d['chosen'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "    rejected_input_ids = tokenizer([d['rejected'] for d in dataset], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    chosen_lengths = np.array([len(prompt) for prompt in chosen_input_ids])\n",
    "    rejected_lengths = np.array([len(prompt) for prompt in rejected_input_ids])\n",
    "    prompt_and_label_lengths = np.maximum(chosen_lengths,rejected_lengths)\n",
    "\n",
    "    print('Input+Output model_max_length (98%, 99%, 99.5%, 99.9%):', np.percentile(prompt_and_label_lengths, [95, 99, 99.5, 99.9]))\n",
    "    print (f'Mean: {(np.mean(chosen_lengths) + np.mean(rejected_lengths))/2:.2f} Num > 2048: {np.sum(prompt_and_label_lengths>2048)} / {len(prompt_and_label_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8846b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51760\n"
     ]
    }
   ],
   "source": [
    "clean_data = load_dataset(\"yahma/alpaca-cleaned\")['train']\n",
    "print (len(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4080b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure the ctx len is loaded, default is 32768\n",
      "INFO 10-14 15:12:49 [utils.py:328] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 4, 'disable_log_stats': True, 'enable_chunked_prefill': False, 'model': 'meta-llama/Llama-3.1-8B-Instruct'}\n",
      "INFO 10-14 15:13:05 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-14 15:13:05 [__init__.py:1815] Using max model len 32768\n",
      "INFO 10-14 15:13:08 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 10-14 15:13:09 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 10-14 15:13:19 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:13:21 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:13:21 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:13:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_a41c1bd5'), local_subscribe_addr='ipc:///tmp/5373208b-75cd-4e94-ab57-677cb1135974', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-14 15:13:30 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-14 15:13:30 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-14 15:13:30 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-14 15:13:30 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-14 15:13:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4779d6c5'), local_subscribe_addr='ipc:///tmp/7a695df0-4a6b-4aa9-a962-bdc8350794ff', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-14 15:13:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_83e9c1a2'), local_subscribe_addr='ipc:///tmp/c3767443-5363-45ad-8dbb-4b8c60c7aaea', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-14 15:13:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6ac89115'), local_subscribe_addr='ipc:///tmp/75c47afe-e558-4da9-8724-7ddd3711c975', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-14 15:13:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_868caaea'), local_subscribe_addr='ipc:///tmp/207ea75a-fa56-44a3-9c3f-aa98e46ebe18', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1014 15:13:34.722143600 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1014 15:13:35.956381231 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1014 15:13:35.581922699 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1014 15:13:35.589385389 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 10-14 15:13:35 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 10-14 15:13:35 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 10-14 15:13:35 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 10-14 15:13:35 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 10-14 15:13:35 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 10-14 15:13:35 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 10-14 15:13:35 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 10-14 15:13:35 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 10-14 15:13:37 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 10-14 15:13:37 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 10-14 15:13:37 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 10-14 15:13:37 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 10-14 15:13:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c361665f'), local_subscribe_addr='ipc:///tmp/def4bdad-3ca2-485a-9549-2b220ec3e732', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 10-14 15:13:37 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 10-14 15:13:37 [parallel_state.py:1165] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 10-14 15:13:37 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 10-14 15:13:37 [parallel_state.py:1165] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "WARNING 10-14 15:13:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 10-14 15:13:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 10-14 15:13:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 10-14 15:13:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:37 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:38 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:38 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:38 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:38 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:38 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:38 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:38 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:38 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  4.25it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.68it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.95it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.79it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:40 [default_loader.py:268] Loading weights took 1.07 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:40 [weight_utils.py:369] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.719109 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:40 [gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 2.482708 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:41 [default_loader.py:268] Loading weights took 1.19 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:41 [gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 3.059974 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:41 [default_loader.py:268] Loading weights took 1.13 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:42 [gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 4.026790 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:42 [default_loader.py:268] Loading weights took 0.99 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:42 [gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 4.608648 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:539] Using cache directory: /export/home2/weijie210/.cache/vllm/torch_compile_cache/23f1c5fc8b/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:539] Using cache directory: /export/home2/weijie210/.cache/vllm/torch_compile_cache/23f1c5fc8b/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:550] Dynamo bytecode transform time: 4.84 s\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:550] Dynamo bytecode transform time: 4.90 s\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:539] Using cache directory: /export/home2/weijie210/.cache/vllm/torch_compile_cache/23f1c5fc8b/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:539] Using cache directory: /export/home2/weijie210/.cache/vllm/torch_compile_cache/23f1c5fc8b/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:550] Dynamo bytecode transform time: 4.90 s\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:48 [backends.py:550] Dynamo bytecode transform time: 4.89 s\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:13:53 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:13:53 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:13:53 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:13:53 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:14:10 [backends.py:215] Compiling a graph for dynamic shape takes 21.97 s\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:14:10 [backends.py:215] Compiling a graph for dynamic shape takes 22.63 s\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:14:10 [backends.py:215] Compiling a graph for dynamic shape takes 22.67 s\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:14:10 [backends.py:215] Compiling a graph for dynamic shape takes 22.62 s\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:14:11 [monitor.py:34] torch.compile takes 26.87 s in total\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:14:11 [monitor.py:34] torch.compile takes 27.47 s in total\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:14:11 [monitor.py:34] torch.compile takes 27.57 s in total\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:14:11 [monitor.py:34] torch.compile takes 27.51 s in total\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:14:13 [gpu_worker.py:298] Available KV cache memory: 61.20 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:14:13 [gpu_worker.py:298] Available KV cache memory: 61.20 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:14:13 [gpu_worker.py:298] Available KV cache memory: 61.20 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:14:13 [gpu_worker.py:298] Available KV cache memory: 61.20 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:864] GPU KV cache size: 2,005,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 61.20x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:864] GPU KV cache size: 2,005,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 61.20x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:864] GPU KV cache size: 2,005,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 61.20x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:864] GPU KV cache size: 2,005,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:14 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 61.20x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:02<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:14:16 [custom_all_reduce.py:203] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:14:16 [custom_all_reduce.py:203] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:14:16 [custom_all_reduce.py:203] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:14:16 [custom_all_reduce.py:203] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:14:17 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.60 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=2258223)\u001b[0;0m INFO 10-14 15:14:17 [gpu_worker.py:391] Free memory on device (78.58/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 3.77 GiB for weight, 4.82 GiB for peak activation, 1.48 GiB for non-torch memory, and 0.6 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64903430963` to fit into requested memory, or `--kv-cache-memory=72757383168` to fully utilize gpu memory. Current kv cache memory in use is 65708737331 bytes.\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:14:17 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.60 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=2258224)\u001b[0;0m INFO 10-14 15:14:17 [gpu_worker.py:391] Free memory on device (78.58/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 3.77 GiB for weight, 4.82 GiB for peak activation, 1.48 GiB for non-torch memory, and 0.6 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64903430963` to fit into requested memory, or `--kv-cache-memory=72757383168` to fully utilize gpu memory. Current kv cache memory in use is 65708737331 bytes.\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:14:17 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.60 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=2258225)\u001b[0;0m INFO 10-14 15:14:17 [gpu_worker.py:391] Free memory on device (78.58/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 3.77 GiB for weight, 4.82 GiB for peak activation, 1.48 GiB for non-torch memory, and 0.6 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64903430963` to fit into requested memory, or `--kv-cache-memory=72757383168` to fully utilize gpu memory. Current kv cache memory in use is 65708737331 bytes.\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:14:17 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.60 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=2258226)\u001b[0;0m INFO 10-14 15:14:17 [gpu_worker.py:391] Free memory on device (78.58/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 3.77 GiB for weight, 4.82 GiB for peak activation, 1.48 GiB for non-torch memory, and 0.6 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64903430963` to fit into requested memory, or `--kv-cache-memory=72757383168` to fully utilize gpu memory. Current kv cache memory in use is 65708737331 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2258182)\u001b[0;0m INFO 10-14 15:14:17 [core.py:218] init engine (profile, create kv cache, warmup model) took 34.23 seconds\n",
      "INFO 10-14 15:14:19 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-14 15:14:19 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    }
   ],
   "source": [
    "model_path = LLAMA_PATH # llama or qwen\n",
    "model,tokenizer,_,_ = load_model(model_path,use_vllm=True)\n",
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=2048, stop=tokenizer.eos_token)\n",
    "\n",
    "\n",
    "preference_data_path = {'train':os.path.join(DATA_DIR, f\"{model.m_name}_metasecalign_train.json\"),\n",
    "                        'val':os.path.join(DATA_DIR, f\"{model.m_name}_metasecalign_val.json\"),}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b9a26",
   "metadata": {},
   "source": [
    "# Adversarial Training\n",
    "\n",
    "Only take samples with data input, since DPO requires the rejected to be the attack's response. Data without input cannot have IPI applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d4dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157\n"
     ]
    }
   ],
   "source": [
    "data_with_injection = []\n",
    "for i,sample in enumerate(clean_data):\n",
    "    if sample.get('input', '').strip() == '':\n",
    "        continue\n",
    "    data_with_injection.append(sample)\n",
    "print (len(data_with_injection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db23f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/19157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 19157/19157 [00:21<00:00, 899.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preference_data = []\n",
    "for i,sample in tqdm(enumerate(data_with_injection),total = len(data_with_injection)):\n",
    "    current_sample = deepcopy(sample)\n",
    "    injected_sample = deepcopy(sample)\n",
    "    instruction = current_sample['instruction']\n",
    "    inpt = current_sample['input']\n",
    "\n",
    "    while injected_sample['instruction'] == current_sample['instruction']:\n",
    "        injected_sample = np.random.choice(data_with_injection, size=1, replace=False)[0]\n",
    "    \n",
    "    injected_prompt = injected_sample['instruction'] + ' ' + injected_sample['input']\n",
    "    if np.random.rand() < 0.9:  # 90% Straightforward Attack, 10% Completion Attack\n",
    "        current_sample['input'] = injected_prompt + ' ' + current_sample['input'] if np.random.rand() < 0.5 else current_sample['input'] + ' ' + injected_prompt\n",
    "    else: \n",
    "        fake_response = current_sample['output']\n",
    "        current_sample['input'] += '\\n\\n' + create_injection_for_completion(fake_response, injected_sample['instruction'], injected_sample['input'])\n",
    "    preference_data.append({\n",
    "                'instruction': current_sample['instruction'],\n",
    "                'input': current_sample['input'],\n",
    "                'chosen_input': instruction + '\\n\\n' + inpt,\n",
    "                'rejected_input': injected_sample['instruction'] + ' ' + injected_sample['input'],\n",
    "            })\n",
    "print (len(preference_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51bcece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9358562d5d9a466b8009955b5320d6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/38314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82032a66b07499694a5ed278d90dada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                  | 0/38314 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4addd2eb3848b98a0086d1a46cbe19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203e2918aba448dfb8193b62eccb13e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input+Output model_max_length (98%, 99%, 99.5%, 99.9%): [ 799.2  1095.88 1267.32 1925.88]\n",
      "Mean: 215.00 Num > 2048: 15 / 18657\n",
      "Input+Output model_max_length (98%, 99%, 99.5%, 99.9%): [ 818.05 1091.07 1168.42 1984.13]\n",
      "Mean: 207.45 Num > 2048: 1 / 500\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=2048, stop=tokenizer.eos_token)\n",
    "conversations = []\n",
    "for sample in preference_data:\n",
    "    conversations.append(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sample[\"chosen_input\"]}], tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "    conversations.append(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sample[\"rejected_input\"]}], tokenize=False, add_generation_prompt=True,enable_thinking=False))\n",
    "outputs = model.generate(conversations, sampling_params,use_tqdm=True)\n",
    "for i in range(len(preference_data)):\n",
    "    sample = preference_data[i]\n",
    "    sample['chosen'] = outputs[2*i].outputs[0].text + tokenizer.eos_token\n",
    "    sample['rejected'] = outputs[2*i+1].outputs[0].text + tokenizer.eos_token\n",
    "\n",
    "val_size = 500\n",
    "preference_data_ids = np.random.permutation(range(len(preference_data)))\n",
    "train_dataset = [preference_data[i] for i in range(len(preference_data)) if i not in preference_data_ids[:val_size]]\n",
    "val_dataset = [preference_data[i] for i in preference_data_ids[:val_size]]\n",
    "\n",
    "jdump(train_dataset, preference_data_path['train'])\n",
    "jdump(val_dataset, preference_data_path['val'])\n",
    "train_dataset = load_dataset('json', data_files=preference_data_path['train'], split='train')\n",
    "val_dataset = load_dataset('json', data_files=preference_data_path['val'], split='train')\n",
    "calculate_length_for_preference_dataset(train_dataset, tokenizer)\n",
    "calculate_length_for_preference_dataset(val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e31d10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
