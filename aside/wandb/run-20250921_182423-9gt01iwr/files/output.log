  0%|          | 0/313 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/export/home2/weijie210/ipi_huawei/aside/fine-tune.py", line 887, in <module>
    main(model_family, emb_type, train_version, model_ix, run_number, user_hparams)
  File "/export/home2/weijie210/ipi_huawei/aside/fine-tune.py", line 765, in main
    trainer.train()
  File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
  File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 3745, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py", line 1267, in compute_loss
    loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
  File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py", line 1219, in get_batch_loss_metrics
    model_output = self.concatenated_forward(model, batch)
  File "/export/home2/weijie210/ipi_huawei/aside/aside_dpo.py", line 77, in concatenated_forward
    concatenated_batch = self.concatenated_inputs(batch, padding_value=self.padding_value)
  File "/export/home2/weijie210/ipi_huawei/aside/aside_dpo.py", line 42, in concatenated_inputs
    if torch.equal(inp_ids[i:i+len(self.start_ids)], torch.tensor(self.start_ids)):
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument other in method wrapper_CUDA__equal)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home2/weijie210/ipi_huawei/aside/fine-tune.py", line 887, in <module>
[rank0]:     main(model_family, emb_type, train_version, model_ix, run_number, user_hparams)
[rank0]:   File "/export/home2/weijie210/ipi_huawei/aside/fine-tune.py", line 765, in main
[rank0]:     trainer.train()
[rank0]:   File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 3745, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py", line 1267, in compute_loss
[rank0]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank0]:   File "/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py", line 1219, in get_batch_loss_metrics
[rank0]:     model_output = self.concatenated_forward(model, batch)
[rank0]:   File "/export/home2/weijie210/ipi_huawei/aside/aside_dpo.py", line 77, in concatenated_forward
[rank0]:     concatenated_batch = self.concatenated_inputs(batch, padding_value=self.padding_value)
[rank0]:   File "/export/home2/weijie210/ipi_huawei/aside/aside_dpo.py", line 42, in concatenated_inputs
[rank0]:     if torch.equal(inp_ids[i:i+len(self.start_ids)], torch.tensor(self.start_ids)):
[rank0]: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument other in method wrapper_CUDA__equal)
