wandb: Detected [huggingface_hub.inference, openai] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|          | 0/809 [00:00<?, ?it/s]
FULL PROMPT: [{'role': 'user', 'content': 'Estimate the total cost of a cross-country road trip from Los Angeles to New York.'}, {'role': 'assistant', 'content': 'Estimating the total cost of a **cross-country road trip from Los Angeles to New York** involves several factors, including:\n\n- **Fuel costs**\n- **Vehicle maintenance and repairs**\n- **Tolls**\n- **Accommodation**\n- **Food**\n- **Miscellaneous expenses (parking, activities, etc.)**\n- **Estimated distance**\n\n---\n\n## üöó **Route Distance**\nThe approximate driving distance from Los Angeles, CA to New York, NY is **around 2,790 miles (4,500 kilometers)**.\n\n---\n\n## ‚úÖ **Estimated Total Cost Breakdown**\n\n### 1. **Fuel Costs**\n- **Vehicle fuel efficiency**: Assume **25 miles per gallon (mpg)** for a mid-size car.\n- **Gas prices**: Estimate $4.50 per gallon (national average).\n  \n  $$\n  \\text{Fuel cost} = \\frac{2,790 \\text{ miles}}{25 \\text{ mpg}} \\times \\$4.50 = 120 \\text{ gallons} \\times \\$4.50 = \\$540\n  $$\n\n---\n\n### 2. **Vehicle Maintenance**\n- Include **tire maintenance, oil changes, minor repairs**.\n- Estimated cost: **\\$100‚Äì\\$200**.\n\n---\n\n### 3. **Tolls**\n- **Estimated tolls**: Varies by route, but on average **\\$150‚Äì\\$300** depending on the route you take.\n\n---\n\n### 4. **Accommodation**\n- **Estimated stay**: 2‚Äì3 nights per state.\n- **Number of states**: ~11 (Los Angeles to New York)\n- **Average cost per night**: \\$100‚Äì\\$200/night\n- **Estimated total**: 20‚Äì25 nights √ó \\$150 = **\\$3,000‚Äì\\$3,750**\n\n---\n\n### 5. **Food**\n- **Daily food cost**: \\$50‚Äì\\$75 per day\n- **Trip duration**: ~20‚Äì25 days\n- **Estimated total**: 20 days √ó \\$60 = **\\$1,200‚Äì\\$1,500**\n\n---\n\n### 6. **Miscellaneous Expenses**\n- **Parking, admission fees, activities**, etc.\n- Estimated total: **\\$500‚Äì\\$1,000**\n\n---\n\n## üìä **Total Estimated Cost**\n\n| Category             | Estimated Cost |\n|----------------------|----------------|\n| Fuel                 | \\$540          |\n| Vehicle Maintenance  | \\$100‚Äì\\$200    |\n| Tolls                | \\$150‚Äì\\$300    |\n| Accommodation        | \\$3,000‚Äì\\$3,750 |\n| Food                 | \\$1,200‚Äì\\$1,500 |\n| Miscellaneous        | \\$500‚Äì\\$1,000   |\n| **Total**            | **\\$5,990‚Äì\\$8,890** |\n\n---\n\n## üìå **Final Estimate**\n- **Average cost**: **\\$7,000‚Äì\\$8,000**\n- **Low-end estimate**: **\\$5,000‚Äì\\$6,000**\n- **High-end estimate**: **\\$9,000‚Äì\\$10,000**\n\n---\n\n## üìù Notes\n- **Route choice** can affect fuel and toll costs (e.g., I-10 vs. I-80).\n- **Vehicle type** (fuel efficiency, size) affects fuel and maintenance costs.\n- **Travel season** (summer vs. off-season) can influence lodging and food prices.\n- **Budgeting extra** for unexpected expenses (e.g., flat tire, car issues) is always wise.\n\nWould you like a breakdown by state or help planning the actual route?'}]
Rotated segment 0: <|im_start|>assistant
<think>

</think>

Estimating the total cost of a **cross-country road trip from Los Angeles to New York** involves several factors, including:

- **Fuel costs**
- **Vehicle maintenance and repairs**
- **Tolls**
- **Accommodation**
- **Food**
- **Miscellaneous expenses (parking, activities, etc.)**
- **Estimated distance**

---

## üöó **Route Distance**
The approximate driving distance from Los Angeles, CA to New York, NY is **around 2,790 miles (4,500 kilometers)**.

---

## ‚úÖ **Estimated Total Cost Breakdown**

### 1. **Fuel Costs**
- **Vehicle fuel efficiency**: Assume **25 miles per gallon (mpg)** for a mid-size car.
- **Gas prices**: Estimate $4.50 per gallon (national average).

  $$
  \text{Fuel cost} = \frac{2,790 \text{ miles}}{25 \text{ mpg}} \times \$4.50 = 120 \text{ gallons} \times \$4.50 = \$540
  $$

---

### 2. **Vehicle Maintenance**
- Include **tire maintenance, oil changes, minor repairs**.
- Estimated cost: **\$100‚Äì\$200**.

---

### 3. **Tolls**
- **Estimated tolls**: Varies by route, but on average **\$150‚Äì\$300** depending on the route you take.

---

### 4. **Accommodation**
- **Estimated stay**: 2‚Äì3 nights per state.
- **Number of states**: ~11 (Los Angeles to New York)
- **Average cost per night**: \$100‚Äì\$200/night
- **Estimated total**: 20‚Äì25 nights √ó \$150 = **\$3,000‚Äì\$3,750**

---

### 5. **Food**
- **Daily food cost**: \$50‚Äì\$75 per day
- **Trip duration**: ~20‚Äì25 days
- **Estimated total**: 20 days √ó \$60 = **\$1,200‚Äì\$1,500**

---

### 6. **Miscellaneous Expenses**
- **Parking, admission fees, activities**, etc.
- Estimated total: **\$500‚Äì\$1,000**

---

## üìä **Total Estimated Cost**

| Category             | Estimated Cost |
|----------------------|----------------|
| Fuel                 | \$540          |
| Vehicle Maintenance  | \$100‚Äì\$200    |
| Tolls                | \$150‚Äì\$300    |
| Accommodation        | \$3,000‚Äì\$3,750 |
| Food                 | \$1,200‚Äì\$1,500 |
| Miscellaneous        | \$500‚Äì\$1,000   |
| **Total**            | **\$5,990‚Äì\$8,890** |

---

## üìå **Final Estimate**
- **Average cost**: **\$7,000‚Äì\$8,000**
- **Low-end estimate**: **\$5,000‚Äì\$6,000**
- **High-end estimate**: **\$9,000‚Äì\$10,000**

---

## üìù Notes
- **Route choice** can affect fuel and toll costs (e.g., I-10 vs. I-80).
- **Vehicle type** (fuel efficiency, size) affects fuel and maintenance costs.
- **Travel season** (summer vs. off-season) can influence lodging and food prices.
- **Budgeting extra** for unexpected expenses (e.g., flat tire, car issues) is always wise.

Would you like a breakdown by state or help planning the actual route?<|im_end|>
  segment_ids = [torch.tensor(feature.pop("segment_ids")) for feature in features]
/export/home2/weijie210/ipi_huawei/aside/fine-tune.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(feature.pop("labels")) for feature in features]
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.

[2025-10-04 14:36:28,086] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|‚ñà‚ñâ        | 161/809 [25:49<1:41:15,  9.38s/it]
[2025-10-04 14:37:08,733] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 19.3342, 'grad_norm': 662.3781127929688, 'learning_rate': 2.727272727272727e-07, 'epoch': 0.01}
[2025-10-04 14:38:33,786] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 17.9235, 'grad_norm': 520.7249755859375, 'learning_rate': 5.757575757575758e-07, 'epoch': 0.02}
{'loss': 13.3017, 'grad_norm': 158.54249572753906, 'learning_rate': 8.787878787878787e-07, 'epoch': 0.04}
{'loss': 9.2524, 'grad_norm': 31.291584014892578, 'learning_rate': 9.99852498153154e-07, 'epoch': 0.05}
[2025-10-04 14:44:08,567] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 7.7375, 'grad_norm': 13.512537002563477, 'learning_rate': 9.989514131188558e-07, 'epoch': 0.06}
{'loss': 7.0867, 'grad_norm': 8.472436904907227, 'learning_rate': 9.972326633975752e-07, 'epoch': 0.07}
{'loss': 6.7985, 'grad_norm': 10.482528686523438, 'learning_rate': 9.946990656181779e-07, 'epoch': 0.09}
{'loss': 6.5268, 'grad_norm': 8.978053092956543, 'learning_rate': 9.913547717549462e-07, 'epoch': 0.1}
{'loss': 6.1833, 'grad_norm': 13.391189575195312, 'learning_rate': 9.872052623234631e-07, 'epoch': 0.11}
{'loss': 5.8004, 'grad_norm': 7.918163299560547, 'learning_rate': 9.822573373993293e-07, 'epoch': 0.12}
{'loss': 5.5066, 'grad_norm': 9.112044334411621, 'learning_rate': 9.765191054744304e-07, 'epoch': 0.14}
{'loss': 5.2204, 'grad_norm': 9.99936580657959, 'learning_rate': 9.69999970169013e-07, 'epoch': 0.15}
{'loss': 4.947, 'grad_norm': 8.306407928466797, 'learning_rate': 9.62710614821352e-07, 'epoch': 0.16}
{'loss': 4.6901, 'grad_norm': 8.260686874389648, 'learning_rate': 9.546629849802561e-07, 'epoch': 0.17}
{'loss': 4.4291, 'grad_norm': 7.859910488128662, 'learning_rate': 9.458702688291071e-07, 'epoch': 0.19}
{'loss': 4.2307, 'grad_norm': 6.5660481452941895, 'learning_rate': 9.363468755735122e-07, 'epoch': 0.2}
***** Running Evaluation *****
  Num examples = 5
  Batch size = 8
FULL PROMPT: [{'role': 'user', 'content': 'Categorize the following toy into its appropriate age group'}, {'role': 'input', 'content': 'Remote Controlled Racecar'}, {'role': 'assistant', 'content': "The toy **Remote Controlled Racecar** is typically appropriate for children aged **6 years and up**. \n\n### Age Group Categorization:\n- **6 years and older**\n\n### Reasoning:\n- **Motor Skills:** Requires fine motor control to operate the remote control.\n- **Cognitive Skills:** Involves understanding basic controls and cause-and-effect.\n- **Safety:** Smaller parts may pose a choking hazard for children under 6, so it's generally not recommended for younger children.\n\nIf the racecar has small parts, it may also be labeled as **not suitable for children under 3 years** due to choking hazards. Always check the packaging for specific age recommendations."}]
Rotated segment 0:
<|im_start|>user
<reference_data>
Remote Controlled Racecar
</reference_data><|im_end|>
<|im_start|>assistant
<think>

</think>

The toy **Remote Controlled Racecar** is typically appropriate for children aged **6 years and up**.

### Age Group Categorization:
- **6 years and older**

### Reasoning:
- **Motor Skills:** Requires fine motor control to operate the remote control.
- **Cognitive Skills:** Involves understanding basic controls and cause-and-effect.
- **Safety:** Smaller parts may pose a choking hazard for children under 6, so it's generally not recommended for younger children.

If the racecar has small parts, it may also be labeled as **not suitable for children under 3 years** due to choking hazards. Always check the packaging for specific age recommendations.<|im_end|>
 20%|‚ñà‚ñâ        | 161/809 [25:49<1:41:15,  9.3 stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161

{'eval_loss': 3.5684762001037598, 'eval_runtime': 0.4981, 'eval_samples_per_second': 10.039, 'eval_steps_per_second': 2.008, 'epoch': 0.2}
Configuration saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161/config.json
Configuration saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161/generation_config.json
Model weights saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161/model.safetensors
chat template saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161/chat_template.jinja
tokenizer config file saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161/tokenizer_config.json
Special tokens file saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161/special_tokens_map.json
/export/home2/weijie210/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
/export/home2/weijie210/ipi_huawei/aside/fine-tune.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  segment_ids = [torch.tensor(feature.pop("segment_ids")) for feature in features]
/export/home2/weijie210/ipi_huawei/aside/fine-tune.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(feature.pop("labels")) for feature in features]
 40%|‚ñà‚ñà‚ñà‚ñâ      | 322/809 [55:28<1:16:28,  9.42s/it] 
{'loss': 4.0874, 'grad_norm': 15.72314739227295, 'learning_rate': 9.261084118279846e-07, 'epoch': 0.21}
{'loss': 4.0512, 'grad_norm': 32.09635925292969, 'learning_rate': 9.151716560403519e-07, 'epoch': 0.22}
{'loss': 3.8636, 'grad_norm': 8.56794548034668, 'learning_rate': 9.035545309958046e-07, 'epoch': 0.23}
{'loss': 3.6588, 'grad_norm': 8.291548728942871, 'learning_rate': 8.912760744456415e-07, 'epoch': 0.25}
{'loss': 3.5893, 'grad_norm': 5.577664375305176, 'learning_rate': 8.783564079088476e-07, 'epoch': 0.26}
{'loss': 3.5124, 'grad_norm': 6.581209659576416, 'learning_rate': 8.648167036976302e-07, 'epoch': 0.27}
{'loss': 3.5004, 'grad_norm': 6.471831798553467, 'learning_rate': 8.506791502209496e-07, 'epoch': 0.28}
{'loss': 3.3604, 'grad_norm': 8.185487747192383, 'learning_rate': 8.359669156229061e-07, 'epoch': 0.3}
{'loss': 3.3556, 'grad_norm': 7.145578861236572, 'learning_rate': 8.207041098155699e-07, 'epoch': 0.31}
{'loss': 3.3051, 'grad_norm': 7.906319618225098, 'learning_rate': 8.049157449684722e-07, 'epoch': 0.32}
{'loss': 3.2444, 'grad_norm': 6.624200344085693, 'learning_rate': 7.886276945195097e-07, 'epoch': 0.33}
{'loss': 3.1864, 'grad_norm': 6.631948471069336, 'learning_rate': 7.718666507744292e-07, 'epoch': 0.35}
{'loss': 3.1431, 'grad_norm': 6.597672939300537, 'learning_rate': 7.546600811643816e-07, 'epoch': 0.36}
{'loss': 3.0839, 'grad_norm': 6.749940395355225, 'learning_rate': 7.37036183233224e-07, 'epoch': 0.37}
{'loss': 3.0605, 'grad_norm': 6.115908145904541, 'learning_rate': 7.190238384283412e-07, 'epoch': 0.38}
{'loss': 3.0083, 'grad_norm': 6.561305046081543, 'learning_rate': 7.006525647707053e-07, 'epoch': 0.4}
***** Running Evaluation *****
  Num examples = 5
  Batch size = 8
 40%|‚ñà‚ñà‚ñà‚ñâ      | 322/809 [55:29<1:16:28,  9.4 stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322
{'eval_loss': 1.9703460931777954, 'eval_runtime': 0.5128, 'eval_samples_per_second': 9.751, 'eval_steps_per_second': 1.95, 'epoch': 0.4}
Configuration saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322/config.json
Configuration saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322/generation_config.json
Model weights saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322/model.safetensors
chat template saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322/chat_template.jinja
tokenizer config file saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322/tokenizer_config.json
Special tokens file saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322/special_tokens_map.json
/export/home2/weijie210/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-161] due to args.save_total_limit
/export/home2/weijie210/ipi_huawei/aside/fine-tune.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  segment_ids = [torch.tensor(feature.pop("segment_ids")) for feature in features]
/export/home2/weijie210/ipi_huawei/aside/fine-tune.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(feature.pop("labels")) for feature in features]
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 483/809 [1:25:09<51:54,  9.55s/it]  
{'loss': 2.9919, 'grad_norm': 6.186993598937988, 'learning_rate': 6.819524684817438e-07, 'epoch': 0.41}
{'loss': 3.007, 'grad_norm': 6.569344997406006, 'learning_rate': 6.629541946462816e-07, 'epoch': 0.42}
{'loss': 2.9526, 'grad_norm': 6.1171875, 'learning_rate': 6.436888769924141e-07, 'epoch': 0.43}
{'loss': 2.8778, 'grad_norm': 5.575580596923828, 'learning_rate': 6.241880868706074e-07, 'epoch': 0.45}
{'loss': 2.8733, 'grad_norm': 6.180209159851074, 'learning_rate': 6.044837815156376e-07, 'epoch': 0.46}
{'loss': 2.9417, 'grad_norm': 6.578696250915527, 'learning_rate': 5.846082516761557e-07, 'epoch': 0.47}
{'loss': 2.7732, 'grad_norm': 5.942425727844238, 'learning_rate': 5.645940686977032e-07, 'epoch': 0.48}
{'loss': 2.8452, 'grad_norm': 7.277872085571289, 'learning_rate': 5.444740311458914e-07, 'epoch': 0.49}
{'loss': 2.8063, 'grad_norm': 6.61274528503418, 'learning_rate': 5.242811110572242e-07, 'epoch': 0.51}
{'loss': 2.764, 'grad_norm': 7.4071431159973145, 'learning_rate': 5.040483999056393e-07, 'epoch': 0.52}
{'loss': 2.7519, 'grad_norm': 5.595850467681885, 'learning_rate': 4.838090543733221e-07, 'epoch': 0.53}
{'loss': 2.7226, 'grad_norm': 6.241373062133789, 'learning_rate': 4.6359624201465597e-07, 'epoch': 0.54}
{'loss': 2.7901, 'grad_norm': 5.314823627471924, 'learning_rate': 4.434430869023579e-07, 'epoch': 0.56}
{'loss': 2.702, 'grad_norm': 7.1230950355529785, 'learning_rate': 4.233826153448684e-07, 'epoch': 0.57}
{'loss': 2.724, 'grad_norm': 6.333163261413574, 'learning_rate': 4.0344770176395606e-07, 'epoch': 0.58}
{'loss': 2.7125, 'grad_norm': 7.708538055419922, 'learning_rate': 3.8367101482122705e-07, 'epoch': 0.59}
***** Running Evaluation *****
  Num examples = 5
  Batch size = 8
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 483/809 [1:25:09<51:54,  9.5 stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-483
{'eval_loss': 1.6399930715560913, 'eval_runtime': 0.5142, 'eval_samples_per_second': 9.723, 'eval_steps_per_second': 1.945, 'epoch': 0.6}
Configuration saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-483/config.json
Configuration saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-483/generation_config.json
Model weights saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-483/model.safetensors
chat template saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-483/chat_template.jinja
tokenizer config file saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-483/tokenizer_config.json
Special tokens file saved in /dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-483/special_tokens_map.json
/export/home2/weijie210/miniconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/dataset/common/huggingface/model/IPI/qwen3_8b_ASIDE_MetaSecAlign_SFT/checkpoint-322] due to args.save_total_limit
/export/home2/weijie210/ipi_huawei/aside/fine-tune.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  segment_ids = [torch.tensor(feature.pop("segment_ids")) for feature in features]
/export/home2/weijie210/ipi_huawei/aside/fine-tune.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(feature.pop("labels")) for feature in features]
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 554/809 [1:40:45<41:22,  9.74s/it]  
{'loss': 2.7358, 'grad_norm': 5.607820510864258, 'learning_rate': 3.640849638818285e-07, 'epoch': 0.61}
{'loss': 2.6811, 'grad_norm': 6.4264373779296875, 'learning_rate': 3.447216459030789e-07, 'epoch': 0.62}
{'loss': 2.6648, 'grad_norm': 5.777795791625977, 'learning_rate': 3.2561279283505884e-07, 'epoch': 0.63}
{'loss': 2.671, 'grad_norm': 5.969051361083984, 'learning_rate': 3.0678971961936764e-07, 'epoch': 0.64}
{'loss': 2.6443, 'grad_norm': 6.674739837646484, 'learning_rate': 2.8828327287125507e-07, 'epoch': 0.66}
[2025-10-04 16:13:42,111] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 2.6084, 'grad_norm': 6.14541482925415, 'learning_rate': 2.7012378032923343e-07, 'epoch': 0.67}
{'loss': 2.5889, 'grad_norm': 6.395424842834473, 'learning_rate': 2.523410011550064e-07, 'epoch': 0.68}
