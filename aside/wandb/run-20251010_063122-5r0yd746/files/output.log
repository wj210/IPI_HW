[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
 20%|███████████████████████████▌                                                                                                               | 29/146 [06:23<25:09, 12.90s/it]
{'loss': 0.6915, 'grad_norm': 5.4005327224731445, 'learning_rate': 6e-07, 'rewards/chosen': 0.0017852783203125, 'rewards/rejected': -0.0003376007080078125, 'rewards/accuracies': 0.3531250059604645, 'rewards/margins': 0.0021209716796875, 'logps/chosen': -171.0, 'logps/rejected': -168.0, 'logits/chosen': -3.421875, 'logits/rejected': -3.734375, 'epoch': 0.07}
{'loss': 0.6608, 'grad_norm': 4.389925956726074, 'learning_rate': 9.977012904914132e-07, 'rewards/chosen': 0.038330078125, 'rewards/rejected': -0.026123046875, 'rewards/accuracies': 0.875, 'rewards/margins': 0.064453125, 'logps/chosen': -166.0, 'logps/rejected': -172.0, 'logits/chosen': -3.46875, 'logits/rejected': -3.4375, 'epoch': 0.14}
***** Running Evaluation *****
  Num examples = 500
  Batch size = 8
 20%|███████████████████████████▌                                                                                                               | 29/146 [06:41<25:09, 12.90s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29                                                                  
{'eval_loss': 0.504562497138977, 'eval_runtime': 18.6342, 'eval_samples_per_second': 26.832, 'eval_steps_per_second': 0.859, 'eval_rewards/chosen': 0.1884765625, 'eval_rewards/rejected': -0.255859375, 'eval_rewards/accuracies': 0.9296875, 'eval_rewards/margins': 0.443359375, 'eval_logps/chosen': -151.0, 'eval_logps/rejected': -211.0, 'eval_logits/chosen': -3.71875, 'eval_logits/rejected': -3.1875, 'epoch': 0.2}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
 40%|███████████████████████████████████████████████████████▏                                                                                   | 58/146 [13:56<18:14, 12.44s/it]
{'loss': 0.5669, 'grad_norm': 5.470951080322266, 'learning_rate': 9.720829318225897e-07, 'rewards/chosen': 0.1640625, 'rewards/rejected': -0.1337890625, 'rewards/accuracies': 0.9437500238418579, 'rewards/margins': 0.298828125, 'logps/chosen': -157.0, 'logps/rejected': -187.0, 'logits/chosen': -3.4375, 'logits/rejected': -3.21875, 'epoch': 0.21}
{'loss': 0.434, 'grad_norm': 5.011024475097656, 'learning_rate': 9.19444094875825e-07, 'rewards/chosen': 0.220703125, 'rewards/rejected': -0.44921875, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 0.671875, 'logps/chosen': -148.0, 'logps/rejected': -217.0, 'logits/chosen': -4.46875, 'logits/rejected': -4.125, 'epoch': 0.27}
{'loss': 0.2354, 'grad_norm': 3.955430507659912, 'learning_rate': 8.427976539882723e-07, 'rewards/chosen': 0.341796875, 'rewards/rejected': -1.296875, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 1.6328125, 'logps/chosen': -133.0, 'logps/rejected': -292.0, 'logits/chosen': -5.71875, 'logits/rejected': -5.25, 'epoch': 0.34}
***** Running Evaluation *****
  Num examples = 500
  Batch size = 8
 40%|███████████████████████████████████████████████████████▏                                                                                   | 58/146 [14:15<18:14, 12.44s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58                                                                  
{'eval_loss': 0.06336572021245956, 'eval_runtime': 18.4768, 'eval_samples_per_second': 27.061, 'eval_steps_per_second': 0.866, 'eval_rewards/chosen': 0.2109375, 'eval_rewards/rejected': -3.953125, 'eval_rewards/accuracies': 0.9921875, 'eval_rewards/margins': 4.15625, 'eval_logps/chosen': -149.0, 'eval_logps/rejected': -580.0, 'eval_logits/chosen': -6.0625, 'eval_logits/rejected': -5.65625, 'epoch': 0.4}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-29] due to args.save_total_limit
 60%|██████████████████████████████████████████████████████████████████████████████████▊                                                        | 87/146 [21:33<12:09, 12.37s/it]
{'loss': 0.0916, 'grad_norm': 2.039888620376587, 'learning_rate': 7.465305999860727e-07, 'rewards/chosen': 0.28125, 'rewards/rejected': -3.171875, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 3.453125, 'logps/chosen': -139.0, 'logps/rejected': -486.0, 'logits/chosen': -5.9375, 'logits/rejected': -5.65625, 'epoch': 0.41}
{'loss': 0.0329, 'grad_norm': 1.385536789894104, 'learning_rate': 6.361529432216559e-07, 'rewards/chosen': 0.216796875, 'rewards/rejected': -4.71875, 'rewards/accuracies': 0.996874988079071, 'rewards/margins': 4.9375, 'logps/chosen': -146.0, 'logps/rejected': -640.0, 'logits/chosen': -6.03125, 'logits/rejected': -5.6875, 'epoch': 0.48}
{'loss': 0.0223, 'grad_norm': 1.9504940509796143, 'learning_rate': 5.179823386575907e-07, 'rewards/chosen': 0.376953125, 'rewards/rejected': -5.46875, 'rewards/accuracies': 1.0, 'rewards/margins': 5.84375, 'logps/chosen': -131.0, 'logps/rejected': -716.0, 'logits/chosen': -5.78125, 'logits/rejected': -5.53125, 'epoch': 0.55}
***** Running Evaluation *****
  Num examples = 500
  Batch size = 8
 60%|██████████████████████████████████████████████████████████████████████████████████▊                                                        | 87/146 [21:52<12:09, 12.37s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87                                                                  
{'eval_loss': 0.02359198033809662, 'eval_runtime': 18.4906, 'eval_samples_per_second': 27.041, 'eval_steps_per_second': 0.865, 'eval_rewards/chosen': 0.314453125, 'eval_rewards/rejected': -6.4375, 'eval_rewards/accuracies': 0.9921875, 'eval_rewards/margins': 6.75, 'eval_logps/chosen': -139.0, 'eval_logps/rejected': -828.0, 'eval_logits/chosen': -6.0, 'eval_logits/rejected': -5.5, 'epoch': 0.6}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-58] due to args.save_total_limit
 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                            | 116/146 [29:06<06:10, 12.36s/it]
{'loss': 0.0136, 'grad_norm': 0.32867109775543213, 'learning_rate': 3.987824840225512e-07, 'rewards/chosen': 0.2578125, 'rewards/rejected': -6.0625, 'rewards/accuracies': 1.0, 'rewards/margins': 6.34375, 'logps/chosen': -139.0, 'logps/rejected': -780.0, 'logits/chosen': -5.90625, 'logits/rejected': -5.53125, 'epoch': 0.62}
{'loss': 0.012, 'grad_norm': 1.2155115604400635, 'learning_rate': 2.853759879426644e-07, 'rewards/chosen': 0.32421875, 'rewards/rejected': -6.375, 'rewards/accuracies': 0.996874988079071, 'rewards/margins': 6.6875, 'logps/chosen': -135.0, 'logps/rejected': -792.0, 'logits/chosen': -6.15625, 'logits/rejected': -5.75, 'epoch': 0.68}
{'loss': 0.015, 'grad_norm': 1.510598063468933, 'learning_rate': 1.8425386620634958e-07, 'rewards/chosen': 0.3515625, 'rewards/rejected': -6.875, 'rewards/accuracies': 1.0, 'rewards/margins': 7.21875, 'logps/chosen': -134.0, 'logps/rejected': -852.0, 'logits/chosen': -6.0, 'logits/rejected': -5.84375, 'epoch': 0.75}
***** Running Evaluation *****
  Num examples = 500
  Batch size = 8
 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                            | 116/146 [29:24<06:10, 12.36s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116                                                                 
{'eval_loss': 0.016690613701939583, 'eval_runtime': 18.4897, 'eval_samples_per_second': 27.042, 'eval_steps_per_second': 0.865, 'eval_rewards/chosen': 0.349609375, 'eval_rewards/rejected': -7.09375, 'eval_rewards/accuracies': 0.9921875, 'eval_rewards/margins': 7.4375, 'eval_logps/chosen': -135.0, 'eval_logps/rejected': -896.0, 'eval_logits/chosen': -6.0625, 'eval_logits/rejected': -5.59375, 'epoch': 0.79}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-87] due to args.save_total_limit
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 145/146 [36:39<00:12, 12.32s/it]
{'loss': 0.0092, 'grad_norm': 0.24193213880062103, 'learning_rate': 1.012040173153521e-07, 'rewards/chosen': 0.396484375, 'rewards/rejected': -6.71875, 'rewards/accuracies': 1.0, 'rewards/margins': 7.125, 'logps/chosen': -129.0, 'logps/rejected': -832.0, 'logits/chosen': -6.09375, 'logits/rejected': -5.65625, 'epoch': 0.82}
{'loss': 0.0111, 'grad_norm': 0.5279165506362915, 'learning_rate': 4.0979942162580386e-08, 'rewards/chosen': 0.40625, 'rewards/rejected': -6.8125, 'rewards/accuracies': 1.0, 'rewards/margins': 7.21875, 'logps/chosen': -131.0, 'logps/rejected': -844.0, 'logits/chosen': -6.09375, 'logits/rejected': -5.78125, 'epoch': 0.89}
{'loss': 0.008, 'grad_norm': 0.3137528896331787, 'learning_rate': 7.028669235575713e-09, 'rewards/chosen': 0.42578125, 'rewards/rejected': -6.71875, 'rewards/accuracies': 1.0, 'rewards/margins': 7.15625, 'logps/chosen': -125.5, 'logps/rejected': -832.0, 'logits/chosen': -6.0625, 'logits/rejected': -5.65625, 'epoch': 0.96}
***** Running Evaluation *****
  Num examples = 500
  Batch size = 8
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 145/146 [36:57<00:12, 12.32s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145                                                                 
{'eval_loss': 0.01650852896273136, 'eval_runtime': 18.4745, 'eval_samples_per_second': 27.064, 'eval_steps_per_second': 0.866, 'eval_rewards/chosen': 0.349609375, 'eval_rewards/rejected': -7.125, 'eval_rewards/accuracies': 0.9921875, 'eval_rewards/margins': 7.46875, 'eval_logps/chosen': -135.0, 'eval_logps/rejected': -900.0, 'eval_logits/chosen': -6.0625, 'eval_logits/rejected': -5.59375, 'epoch': 0.99}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-116] due to args.save_total_limit
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [38:25<00:00, 40.49s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once


Training completed. Do not forget to share your model on huggingface.co/models =)


/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading best model from /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145 (score: 0.01650852896273136).
Attempting to resume from /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-145
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [39:50<00:00, 40.49s/it]Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/checkpoint-146] due to args.save_total_limit
{'train_runtime': 2392.4839, 'train_samples_per_second': 7.798, 'train_steps_per_second': 0.061, 'train_loss': 0.19263166883220412, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [40:04<00:00, 16.47s/it]
Custom impl., saving last checkpoint
 stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/special_tokens_map.json
Processing zero checkpoint '/mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01/global_step145'
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 1675.04it/s]
Detected checkpoint of type zero stage ZeroStageEnum.weights, world_size: 4
Parsing checkpoint created by deepspeed==0.17.6
Gathering sharded weights: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 399/399 [00:00<00:00, 1027334.13it/s]
Reconstructed Trainable fp32 state dict with 399 params 8188564480 elements
Saving checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:23<00:00, 11.90s/it]
Saved merged FP32 checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_dpo_Vanilla_MetaSecAlign_lr1e-6_b0.01
