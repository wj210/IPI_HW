wandb: Detected [huggingface_hub.inference, openai] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|          | 0/2246 [00:00<?, ?it/s]
Prompt : <|im_start|>system
# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"name": "jobs_for_jobsapi", "description": "This is the subfunction for tool \"jobsapi\", you can use this tool.The description of this function is: \"get's jobs\"", "parameters": {"type": "object", "properties": {}, "required": [], "optional": []}}
{"name": "albums_for_march4", "description": "This is the subfunction for tool \"march4\", you can use this tool.The description of this function is: \"njnj\"", "parameters": {"type": "object", "properties": {}, "required": [], "optional": []}}
{"name": "get_anything_for_httpbin", "description": "This is the subfunction for tool \"httpbin\", you can use this tool.The description of this function is: \"Forward to httpbin get_anything_for_httpbin endpoint\"", "parameters": {"type": "object", "properties": {}, "required": [], "optional": []}}
{"name": "getinventory_for_privatepublicapi", "description": "This is the subfunction for tool \"privatepublicapi\", you can use this tool.The description of this function is: \"Returns a map of status codes to quantities\"", "parameters": {"type": "object", "properties": {}, "required": [], "optional": []}}
{"name": "getorderbyid_for_privatepublicapi", "description": "This is the subfunction for tool \"privatepublicapi\", you can use this tool.The description of this function is: \"For valid response try integer IDs with value >= 1 and <= 10. Other values will generated exceptions\"", "parameters": {"type": "object", "properties": {"orderid": {"type": "integer", "description": "ID of pet that needs to be fetched", "example_value": 0}}, "required": ["orderid"], "optional": []}}
{"name": "logoutuser_for_privatepublicapi", "description": "This is the subfunction for tool \"privatepublicapi\", you can use this tool.The description of this function is: \"Logs out current logged in user session\"", "parameters": {"type": "object", "properties": {}, "required": [], "optional": []}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call><|im_end|>
<|im_start|>user
I am passionate about photography and I want to improve my skills. Can you suggest some photography courses or workshops in my area and provide information about their curriculum and instructors? Additionally, it would be helpful to know if any of these courses offer hands-on practice and feedback.
Begin!
<|im_end|>
<|im_start|>assistant
<tool_call>
{"name": "jobs_for_jobsapi", "arguments": {}}
</tool_call><|im_end|>
<|im_start|>user
<tool_response>
{"error": "", "response": "{'message': \"Endpoint '/api/job' does not exist\"}"}
</tool_response><|im_end|>
<|im_start|>assistant
<think>

</think>

<tool_call>
{"name": "albums_for_march4", "arguments": {}}
</tool_call><|im_end|>

Rotated chunk 0:
<|im_start|>assistant
<tool_call>
{"name": "jobs_for_jobsapi", "arguments": {}}
</tool_call><|im_end|>
Rotated chunk 1: <|im_start|>user
<tool_response>
{"error": "", "response": "{'message': \"Endpoint '/api/job' does not exist\"}"}
</tool_response><|im_end|>
<|im_start|>assistant
<think>

</think>

<tool_call>
{"name": "albums_for_march4", "arguments": {}}
</tool_call><|im_end|>
[2025-09-28 11:25:51,448] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 10/2246 [03:02<10:44:08, 17.28s/it]2025-09-28 11:28:32,331 - INFO - Step 10: Train Loss = 23.5046
[2025-09-28 11:26:12,325] [WARNING] [stage3.py:2171:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2025-09-28 11:26:31,085] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2025-09-28 11:26:49,581] [WARNING] [stage3.py:2171:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2025-09-28 11:27:58,240] [WARNING] [stage3.py:2171:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 23.5046, 'grad_norm': 384.87432861328125, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  1%|          | 20/2246 [05:55<10:33:12, 17.07s/it]2025-09-28 11:31:25,147 - INFO - Step 20: Train Loss = 11.0250
[2025-09-28 11:30:33,411] [WARNING] [stage3.py:2171:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2025-09-28 11:31:09,765] [WARNING] [stage3.py:2171:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 11.025, 'grad_norm': 21.914718627929688, 'learning_rate': 4.222222222222223e-06, 'epoch': 0.01}
  1%|          | 26/2246 [07:36<10:23:18, 16.85s/it]
