[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                   | 0/1210 [00:00<?, ?it/s]
Tool Prompt : <|im_start|>system
# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"type": "function", "function": {"name": "citeassist-mcp-get_citeas_data", "description": "Retrieve BibTeX-formatted citation for the specified `resource` (e.g., DOI, URL, keyword) from the CiteAs", "parameters": {"properties": {"resource": {"title": "Resource", "type": "string"}}, "required": ["resource"], "title": "get_citeas_dataArguments", "type": "object"}}}
{"type": "function", "function": {"name": "citeassist-mcp-get_scholar_data", "description": "Retrieve `results` BibTeX-formatted citations for publications matching the `query` from the Google Scholar", "parameters": {"properties": {"query": {"title": "Query", "type": "string"}, "results": {"default": 2, "title": "Results", "type": "integer"}}, "required": ["query"], "title": "get_scholar_dataArguments", "type": "object"}}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call><|im_end|>
<|im_start|>user
I need to format a citation for the article I just read. Can you help me get a BibTeX citation for the following DOI: 10.1038/nature12373?<|im_end|>
<|im_start|>assistant
<think>

</think>

<tool_call>
{"name": "citeassist-mcp-get_citeas_data", "arguments": {"resource": "10.1038/nature12373"}}
</tool_call><|im_end|>

----------------------------------------------------------------------------------------------------------------------------------------------------------------
Assistant loss chunk 0: <|im_start|>assistant
<think>

</think>

<tool_call>
{"name": "citeassist-mcp-get_citeas_data", "arguments": {"resource": "10.1038/nature12373"}}
</tool_call><|im_end|>
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Input Prompt : <|im_start|>user
Rewrite the sentence to make its meaning clearer without changing its main point.<|im_end|>
<|im_start|>user
<reference_data>
It was clear the politician didn't have the solutions to address the issue.
</reference_data><|im_end|>
<|im_start|>assistant
<think>

</think>

It was clear that the politician did not have the solutions needed to address the issue.<|im_end|>

Rotated chunk 0:
<|im_start|>user
<reference_data>
It was clear the politician didn't have the solutions to address the issue.
</reference_data><|im_end|>
<|im_start|>assistant
<think>

</think>

It was clear that the politician did not have the solutions needed to address the issue.<|im_end|>
----------------------------------------------------------------------------------------------------------------------------------------------------------------
 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                          | 241/1210 [1:12:35<4:52:31, 18.11s/it]
[2025-10-10 00:18:26,540] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3881, 'grad_norm': 5.069072723388672, 'learning_rate': 1.8367346938775512e-06, 'epoch': 0.02}
{'loss': 0.2122, 'grad_norm': 1.9797357320785522, 'learning_rate': 3.877551020408164e-06, 'epoch': 0.03}
[2025-10-10 00:24:30,575] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1426, 'grad_norm': 2.1318066120147705, 'learning_rate': 5.918367346938776e-06, 'epoch': 0.05}
{'loss': 0.1308, 'grad_norm': 1.5066699981689453, 'learning_rate': 7.959183673469388e-06, 'epoch': 0.07}
{'loss': 0.1258, 'grad_norm': 1.4396904706954956, 'learning_rate': 1e-05, 'epoch': 0.08}
{'loss': 0.1224, 'grad_norm': 1.4305387735366821, 'learning_rate': 9.998169588166204e-06, 'epoch': 0.1}
{'loss': 0.127, 'grad_norm': 1.2444202899932861, 'learning_rate': 9.992679692827812e-06, 'epoch': 0.12}
{'loss': 0.1318, 'grad_norm': 1.2789855003356934, 'learning_rate': 9.983534333492575e-06, 'epoch': 0.13}
{'loss': 0.1282, 'grad_norm': 1.2883508205413818, 'learning_rate': 9.97074020607008e-06, 'epoch': 0.15}
{'loss': 0.1333, 'grad_norm': 1.2697666883468628, 'learning_rate': 9.954306677969218e-06, 'epoch': 0.17}
{'loss': 0.1319, 'grad_norm': 1.3851124048233032, 'learning_rate': 9.934245781239714e-06, 'epoch': 0.18}
{'loss': 0.1325, 'grad_norm': 1.179749846458435, 'learning_rate': 9.910572203762676e-06, 'epoch': 0.2}
{'loss': 0.1379, 'grad_norm': 1.4505081176757812, 'learning_rate': 9.883303278496646e-06, 'epoch': 0.22}
{'loss': 0.136, 'grad_norm': 1.39218008518219, 'learning_rate': 9.852458970787027e-06, 'epoch': 0.23}
{'loss': 0.1374, 'grad_norm': 1.3507800102233887, 'learning_rate': 9.818061863748153e-06, 'epoch': 0.25}
{'loss': 0.1303, 'grad_norm': 1.2924270629882812, 'learning_rate': 9.780137141728737e-06, 'epoch': 0.26}
{'loss': 0.1325, 'grad_norm': 1.1384031772613525, 'learning_rate': 9.738712571872765e-06, 'epoch': 0.28}
{'loss': 0.137, 'grad_norm': 1.3876183032989502, 'learning_rate': 9.693818483789386e-06, 'epoch': 0.3}
{'loss': 0.1337, 'grad_norm': 1.217833399772644, 'learning_rate': 9.645487747346643e-06, 'epoch': 0.31}
{'loss': 0.1373, 'grad_norm': 1.3075624704360962, 'learning_rate': 9.5937557486053e-06, 'epoch': 0.33}
{'loss': 0.1314, 'grad_norm': 1.2979614734649658, 'learning_rate': 9.538660363910433e-06, 'epoch': 0.35}
[2025-10-10 01:22:40,461] [WARNING] [stage3.py:2171:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1347, 'grad_norm': 1.2691293954849243, 'learning_rate': 9.480241932159692e-06, 'epoch': 0.36}
{'loss': 0.1392, 'grad_norm': 1.30354642868042, 'learning_rate': 9.418543225268598e-06, 'epoch': 0.38}
{'loss': 0.1317, 'grad_norm': 1.1913301944732666, 'learning_rate': 9.353609416854432e-06, 'epoch': 0.4}
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 8
Tool Prompt : <|im_start|>user
Suggest three cities in California that could be great for a family vacation.<|im_end|>
<|im_start|>assistant
<think>

</think>

Certainly! California offers a wide variety of cities that are perfect for a family vacation, with something to suit every age and interest. Here are three great options:

### 1. **San Diego**
- **Why it's great for families:** San Diego is known for its mild climate, beautiful beaches, and family-friendly attractions.
- **Top attractions:**
  - **San Diego Zoo:** One of the world's most famous zoos, home to thousands of animals.
  - **Balboa Park:** Features museums, gardens, and the world's largest urban park.
  - **La Jolla Cove:** A scenic beach with great snorkeling and tide pool exploration.
  - **Children's Museum of San Diego:** Interactive exhibits for kids of all ages.
- **Family-friendly activities:** Beaches, outdoor adventures, and cultural experiences.

### 2. **Los Angeles**
- **Why it's great for families:** LA offers a mix of entertainment, culture, and outdoor activities, with plenty of options for all ages.
- **Top attractions:**
  - **Griffith Observatory:** Offers stunning views of the city and the Hollywood sign, plus educational exhibits.
  - **Santa Monica Pier:** A fun spot with rides, games, and the famous Ferris wheel.
  - **Disneyland Resort (Anaheim):** Just a short drive away, it's a must-visit for families with kids.
  - **The Getty Center:** A beautiful museum with art and gardens.
- **Family-friendly activities:** Theme parks, museums, and beach days.

### 3. **Sacramento**
- **Why it's great for families:** As the capital of California, Sacramento offers a mix of history, culture, and family-friendly attractions without the crowds of larger cities.
- **Top attractions:**
  - **California State Capitol Museum:** Offers a fascinating look into the state's history and government.
  - **Sutter‚Äôs Fort State Historic Park:** A living history museum with costumed interpreters and interactive exhibits.
  - **Discovery Children‚Äôs Museum:** A hands-on museum designed for young children.
  - **Sacramento Riverwalk:** A scenic path along the river with great views and picnic spots.
- **Family-friendly activities:** History, museums, and outdoor exploration.

Each of these cities offers a unique experience, so depending on your family's interests, you can choose the one that best fits your vacation goals.<|im_end|>

----------------------------------------------------------------------------------------------------------------------------------------------------------------
Assistant loss chunk 0: <|im_start|>assistant
<think>

</think>

Certainly! California offers a wide variety of cities that are perfect for a family vacation, with something to suit every age and interest. Here are three great options:

### 1. **San Diego**
- **Why it's great for families:** San Diego is known for its mild climate, beautiful beaches, and family-friendly attractions.
- **Top attractions:**
  - **San Diego Zoo:** One of the world's most famous zoos, home to thousands of animals.
  - **Balboa Park:** Features museums, gardens, and the world's largest urban park.
  - **La Jolla Cove:** A scenic beach with great snorkeling and tide pool exploration.
  - **Children's Museum of San Diego:** Interactive exhibits for kids of all ages.
- **Family-friendly activities:** Beaches, outdoor adventures, and cultural experiences.

### 2. **Los Angeles**
- **Why it's great for families:** LA offers a mix of entertainment, culture, and outdoor activities, with plenty of options for all ages.
- **Top attractions:**
  - **Griffith Observatory:** Offers stunning views of the city and the Hollywood sign, plus educational exhibits.
  - **Santa Monica Pier:** A fun spot with rides, games, and the famous Ferris wheel.
  - **Disneyland Resort (Anaheim):** Just a short drive away, it's a must-visit for families with kids.
  - **The Getty Center:** A beautiful museum with art and gardens.
- **Family-friendly activities:** Theme parks, museums, and beach days.

### 3. **Sacramento**
- **Why it's great for families:** As the capital of California, Sacramento offers a mix of history, culture, and family-friendly attractions without the crowds of larger cities.
- **Top attractions:**
  - **California State Capitol Museum:** Offers a fascinating look into the state's history and government.
  - **Sutter‚Äôs Fort State Historic Park:** A living history museum with costumed interpreters and interactive exhibits.
  - **Discovery Children‚Äôs Museum:** A hands-on museum designed for young children.
  - **Sacramento Riverwalk:** A scenic path along the river with great views and picnic spots.
- **Family-friendly activities:** History, museums, and outdoor exploration.

Each of these cities offers a unique experience, so depending on your family's interests, you can choose the one that best fits your vacation goals.<|im_end|>
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Input Prompt : <|im_start|>user
Generate a response that expresses sympathy<|im_end|>
<|im_start|>user
<reference_data>
Situation: Your friend's pet died
</reference_data><|im_end|>
<|im_start|>assistant
<think>

</think>

I'm so sorry to hear about your friend's loss. Losing a pet is incredibly painful, as they become such a cherished part of our lives. I hope your friend finds comfort in the happy memories they shared with their beloved pet, and that they're able to heal over time. Let them know they're not alone in this grief‚Äîit's a sign of how much their pet meant to them.<|im_end|>

Rotated chunk 0:
<|im_start|>user
<reference_data>
Situation: Your friend's pet died
</reference_data><|im_end|>
<|im_start|>assistant
<think>

</think>

I'm so sorry to hear about your friend's loss. Losing a pet is incredibly painful, as they become such a cherished part of our lives. I hope your friend finds comfort in the happy memories they shared with their beloved pet, and that they're able to heal over time. Let them know they're not alone in this grief‚Äîit's a sign of how much their pet meant to them.<|im_end|>
----------------------------------------------------------------------------------------------------------------------------------------------------------------
 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                          | 241/1210 [1:13:06<4:52:31, 18.11s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241                                                                                 
{'eval_loss': 0.1257028728723526, 'eval_runtime': 31.7009, 'eval_samples_per_second': 31.545, 'eval_steps_per_second': 1.009, 'epoch': 0.4}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                | 482/1210 [2:25:19<3:28:07, 17.15s/it]
{'loss': 0.1406, 'grad_norm': 1.5135596990585327, 'learning_rate': 9.285488049161735e-06, 'epoch': 0.41}
{'loss': 0.1342, 'grad_norm': 1.4294860363006592, 'learning_rate': 9.214228998253526e-06, 'epoch': 0.43}
{'loss': 0.1351, 'grad_norm': 1.1734257936477661, 'learning_rate': 9.139884437493828e-06, 'epoch': 0.45}
{'loss': 0.1389, 'grad_norm': 1.3181217908859253, 'learning_rate': 9.062508799348155e-06, 'epoch': 0.46}
{'loss': 0.1384, 'grad_norm': 1.305091381072998, 'learning_rate': 8.982158735529991e-06, 'epoch': 0.48}
{'loss': 0.1363, 'grad_norm': 1.370421051979065, 'learning_rate': 8.8988930755224e-06, 'epoch': 0.5}
{'loss': 0.1381, 'grad_norm': 1.3060132265090942, 'learning_rate': 8.812772783505158e-06, 'epoch': 0.51}
{'loss': 0.1366, 'grad_norm': 1.1608490943908691, 'learning_rate': 8.72386091371891e-06, 'epoch': 0.53}
{'loss': 0.1359, 'grad_norm': 1.4334948062896729, 'learning_rate': 8.632222564299111e-06, 'epoch': 0.55}
{'loss': 0.136, 'grad_norm': 1.3081586360931396, 'learning_rate': 8.537924829613444e-06, 'epoch': 0.56}
{'loss': 0.1366, 'grad_norm': 1.470873475074768, 'learning_rate': 8.441036751137697e-06, 'epoch': 0.58}
{'loss': 0.1369, 'grad_norm': 1.3340482711791992, 'learning_rate': 8.341629266906024e-06, 'epoch': 0.6}
{'loss': 0.1339, 'grad_norm': 1.3199702501296997, 'learning_rate': 8.239775159572632e-06, 'epoch': 0.61}
{'loss': 0.1339, 'grad_norm': 1.2541749477386475, 'learning_rate': 8.135549003122871e-06, 'epoch': 0.63}
{'loss': 0.1317, 'grad_norm': 1.1973398923873901, 'learning_rate': 8.029027108272806e-06, 'epoch': 0.65}
{'loss': 0.1327, 'grad_norm': 1.3598607778549194, 'learning_rate': 7.920287466597193e-06, 'epoch': 0.66}
{'loss': 0.135, 'grad_norm': 1.141806721687317, 'learning_rate': 7.809409693426803e-06, 'epoch': 0.68}
{'loss': 0.133, 'grad_norm': 1.3514267206192017, 'learning_rate': 7.69647496955688e-06, 'epoch': 0.7}
{'loss': 0.1287, 'grad_norm': 1.3134969472885132, 'learning_rate': 7.581565981809435e-06, 'epoch': 0.71}
{'loss': 0.1329, 'grad_norm': 1.367258906364441, 'learning_rate': 7.464766862492856e-06, 'epoch': 0.73}
{'loss': 0.1301, 'grad_norm': 1.223097562789917, 'learning_rate': 7.3461631278032175e-06, 'epoch': 0.74}
{'loss': 0.1338, 'grad_norm': 1.3852359056472778, 'learning_rate': 7.22584161521232e-06, 'epoch': 0.76}
{'loss': 0.1331, 'grad_norm': 1.3034456968307495, 'learning_rate': 7.103890419888367e-06, 'epoch': 0.78}
{'loss': 0.1327, 'grad_norm': 1.2156413793563843, 'learning_rate': 6.980398830195785e-06, 'epoch': 0.79}
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 8
 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                | 482/1210 [2:25:51<3:28:07, 17.15s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482                                                                                 
{'eval_loss': 0.1209324523806572, 'eval_runtime': 31.7813, 'eval_samples_per_second': 31.465, 'eval_steps_per_second': 1.007, 'epoch': 0.8}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-241] due to args.save_total_limit
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 723/1210 [3:38:17<2:30:53, 18.59s/it]
{'loss': 0.1292, 'grad_norm': 1.1515026092529297, 'learning_rate': 6.855457262321433e-06, 'epoch': 0.81}
{'loss': 0.1262, 'grad_norm': 1.2503674030303955, 'learning_rate': 6.7291571940750575e-06, 'epoch': 0.83}
{'loss': 0.1334, 'grad_norm': 1.3264484405517578, 'learning_rate': 6.601591097912472e-06, 'epoch': 0.84}
{'loss': 0.134, 'grad_norm': 1.2779303789138794, 'learning_rate': 6.472852373230478e-06, 'epoch': 0.86}
{'loss': 0.1293, 'grad_norm': 1.2698885202407837, 'learning_rate': 6.3430352779831275e-06, 'epoch': 0.88}
{'loss': 0.1264, 'grad_norm': 1.2761386632919312, 'learning_rate': 6.212234859669366e-06, 'epoch': 0.89}
{'loss': 0.1314, 'grad_norm': 1.2093344926834106, 'learning_rate': 6.080546885742611e-06, 'epoch': 0.91}
{'loss': 0.1295, 'grad_norm': 1.3335527181625366, 'learning_rate': 5.948067773493205e-06, 'epoch': 0.93}
{'loss': 0.1313, 'grad_norm': 1.2347888946533203, 'learning_rate': 5.814894519455061e-06, 'epoch': 0.94}
{'loss': 0.1301, 'grad_norm': 1.3109745979309082, 'learning_rate': 5.681124628388235e-06, 'epoch': 0.96}
{'loss': 0.1256, 'grad_norm': 1.2552387714385986, 'learning_rate': 5.546856041889374e-06, 'epoch': 0.98}
{'loss': 0.1229, 'grad_norm': 1.2155771255493164, 'learning_rate': 5.412187066682327e-06, 'epoch': 0.99}
{'loss': 0.092, 'grad_norm': 1.0228716135025024, 'learning_rate': 5.2772163026414455e-06, 'epoch': 1.01}
{'loss': 0.0594, 'grad_norm': 0.9631519317626953, 'learning_rate': 5.142042570600212e-06, 'epoch': 1.02}
{'loss': 0.0597, 'grad_norm': 0.9310601353645325, 'learning_rate': 5.006764839998128e-06, 'epoch': 1.04}
{'loss': 0.0569, 'grad_norm': 0.987661600112915, 'learning_rate': 4.871482156418769e-06, 'epoch': 1.06}
{'loss': 0.0587, 'grad_norm': 1.2111599445343018, 'learning_rate': 4.736293569072108e-06, 'epoch': 1.07}
{'loss': 0.0576, 'grad_norm': 0.9813777804374695, 'learning_rate': 4.6012980582741725e-06, 'epoch': 1.09}
{'loss': 0.058, 'grad_norm': 0.9111763834953308, 'learning_rate': 4.466594462977156e-06, 'epoch': 1.11}
{'loss': 0.0612, 'grad_norm': 0.8098092675209045, 'learning_rate': 4.332281408403011e-06, 'epoch': 1.12}
{'loss': 0.0571, 'grad_norm': 1.6437320709228516, 'learning_rate': 4.198457233833546e-06, 'epoch': 1.14}
{'loss': 0.0573, 'grad_norm': 1.1331470012664795, 'learning_rate': 4.065219920609877e-06, 'epoch': 1.16}
{'loss': 0.054, 'grad_norm': 0.8297280073165894, 'learning_rate': 3.932667020393933e-06, 'epoch': 1.17}
{'loss': 0.0534, 'grad_norm': 0.9256090521812439, 'learning_rate': 3.8008955837445742e-06, 'epoch': 1.19}
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 8
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 723/1210 [3:38:49<2:30:53, 18.59s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723                                                                                 
{'eval_loss': 0.11822404712438583, 'eval_runtime': 31.7592, 'eval_samples_per_second': 31.487, 'eval_steps_per_second': 1.008, 'epoch': 1.2}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-482] due to args.save_total_limit
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 964/1210 [4:50:40<1:08:58, 16.82s/it]
{'loss': 0.0578, 'grad_norm': 1.0009657144546509, 'learning_rate': 3.6700020890606068e-06, 'epoch': 1.21}
{'loss': 0.0588, 'grad_norm': 0.8572353720664978, 'learning_rate': 3.540082371942682e-06, 'epoch': 1.22}
{'loss': 0.0579, 'grad_norm': 0.953553318977356, 'learning_rate': 3.411231555025861e-06, 'epoch': 1.24}
{'loss': 0.0573, 'grad_norm': 1.055754542350769, 'learning_rate': 3.283543978334177e-06, 'epoch': 1.26}
{'loss': 0.0566, 'grad_norm': 0.9265384674072266, 'learning_rate': 3.1571131302081916e-06, 'epoch': 1.27}
{'loss': 0.0556, 'grad_norm': 0.9073215126991272, 'learning_rate': 3.0320315788561334e-06, 'epoch': 1.29}
{'loss': 0.0577, 'grad_norm': 0.950713574886322, 'learning_rate': 2.9083909045787116e-06, 'epoch': 1.31}
{'loss': 0.0566, 'grad_norm': 0.8657110333442688, 'learning_rate': 2.786281632717264e-06, 'epoch': 1.32}
{'loss': 0.0563, 'grad_norm': 0.9287905097007751, 'learning_rate': 2.6657931673742834e-06, 'epoch': 1.34}
{'loss': 0.0558, 'grad_norm': 0.915273129940033, 'learning_rate': 2.547013725954887e-06, 'epoch': 1.36}
{'loss': 0.0528, 'grad_norm': 0.8666272163391113, 'learning_rate': 2.430030274577151e-06, 'epoch': 1.37}
{'loss': 0.0538, 'grad_norm': 0.9840252995491028, 'learning_rate': 2.314928464398581e-06, 'epoch': 1.39}
{'loss': 0.0548, 'grad_norm': 0.8599733710289001, 'learning_rate': 2.20179256890535e-06, 'epoch': 1.41}
{'loss': 0.0543, 'grad_norm': 0.8941431641578674, 'learning_rate': 2.0907054222102367e-06, 'epoch': 1.42}
{'loss': 0.0533, 'grad_norm': 0.9309497475624084, 'learning_rate': 1.9817483584043954e-06, 'epoch': 1.44}
{'loss': 0.0532, 'grad_norm': 1.00978684425354, 'learning_rate': 1.8750011520074158e-06, 'epoch': 1.46}
{'loss': 0.0538, 'grad_norm': 0.9728842973709106, 'learning_rate': 1.7705419595592193e-06, 'epoch': 1.47}
{'loss': 0.0501, 'grad_norm': 0.8443893790245056, 'learning_rate': 1.6684472623966108e-06, 'epoch': 1.49}
{'loss': 0.0535, 'grad_norm': 0.9182741641998291, 'learning_rate': 1.5687918106563326e-06, 'epoch': 1.5}
{'loss': 0.0515, 'grad_norm': 0.9920114874839783, 'learning_rate': 1.47164856854565e-06, 'epoch': 1.52}
{'loss': 0.0523, 'grad_norm': 0.8326122164726257, 'learning_rate': 1.3770886609205381e-06, 'epoch': 1.54}
{'loss': 0.0519, 'grad_norm': 0.8177009224891663, 'learning_rate': 1.285181321210562e-06, 'epoch': 1.55}
{'loss': 0.0518, 'grad_norm': 0.9634431600570679, 'learning_rate': 1.1959938407286099e-06, 'epoch': 1.57}
{'loss': 0.0506, 'grad_norm': 0.8862461447715759, 'learning_rate': 1.1095915194025642e-06, 'epoch': 1.59}
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 8
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 964/1210 [4:51:12<1:08:58, 16.82s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964                                                                                 
{'eval_loss': 0.10968249291181564, 'eval_runtime': 31.6987, 'eval_samples_per_second': 31.547, 'eval_steps_per_second': 1.01, 'epoch': 1.59}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-723] due to args.save_total_limit
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1205/1210 [6:03:49<01:23, 16.65s/it]
{'loss': 0.0532, 'grad_norm': 0.9301726818084717, 'learning_rate': 1.0260376179649905e-06, 'epoch': 1.6}
{'loss': 0.0528, 'grad_norm': 0.7540257573127747, 'learning_rate': 9.453933116358715e-07, 'epoch': 1.62}
{'loss': 0.0531, 'grad_norm': 0.8592254519462585, 'learning_rate': 8.677176453322611e-07, 'epoch': 1.64}
{'loss': 0.052, 'grad_norm': 0.8303472995758057, 'learning_rate': 7.930674904376762e-07, 'epoch': 1.65}
{'loss': 0.0502, 'grad_norm': 0.8676797747612, 'learning_rate': 7.214975031628856e-07, 'epoch': 1.67}
{'loss': 0.0525, 'grad_norm': 1.0275012254714966, 'learning_rate': 6.530600845285478e-07, 'epoch': 1.69}
{'loss': 0.0511, 'grad_norm': 0.8770557641983032, 'learning_rate': 5.87805341999042e-07, 'epoch': 1.7}
{'loss': 0.0509, 'grad_norm': 0.7725171446800232, 'learning_rate': 5.25781052795541e-07, 'epoch': 1.72}
{'loss': 0.0502, 'grad_norm': 0.8488492965698242, 'learning_rate': 4.6703262891522214e-07, 'epoch': 1.74}
{'loss': 0.0527, 'grad_norm': 0.9404478073120117, 'learning_rate': 4.1160308388220103e-07, 'epoch': 1.75}
{'loss': 0.0489, 'grad_norm': 0.9805219769477844, 'learning_rate': 3.595330012545445e-07, 'epoch': 1.77}
{'loss': 0.0514, 'grad_norm': 0.952044665813446, 'learning_rate': 3.108605049104246e-07, 'epoch': 1.79}
{'loss': 0.0461, 'grad_norm': 0.901268482208252, 'learning_rate': 2.65621231135158e-07, 'epoch': 1.8}
{'loss': 0.0482, 'grad_norm': 0.784065306186676, 'learning_rate': 2.2384830252957068e-07, 'epoch': 1.82}
{'loss': 0.0506, 'grad_norm': 0.8871330618858337, 'learning_rate': 1.8557230375880364e-07, 'epoch': 1.84}
{'loss': 0.0482, 'grad_norm': 0.7940372228622437, 'learning_rate': 1.5082125915929946e-07, 'epoch': 1.85}
{'loss': 0.0483, 'grad_norm': 0.9703727960586548, 'learning_rate': 1.196206122203647e-07, 'epoch': 1.87}
{'loss': 0.051, 'grad_norm': 0.9892989993095398, 'learning_rate': 9.199320695535086e-08, 'epoch': 1.89}
{'loss': 0.0493, 'grad_norm': 0.7840486168861389, 'learning_rate': 6.795927117607238e-08, 'epoch': 1.9}
{'loss': 0.0489, 'grad_norm': 0.8606060743331909, 'learning_rate': 4.753640168271456e-08, 'epoch': 1.92}
{'loss': 0.0497, 'grad_norm': 0.8453301191329956, 'learning_rate': 3.073955138007734e-08, 'epoch': 1.94}
{'loss': 0.0487, 'grad_norm': 1.0222855806350708, 'learning_rate': 1.758101832958603e-08, 'epoch': 1.95}
{'loss': 0.0486, 'grad_norm': 0.8698379397392273, 'learning_rate': 8.07043674508623e-09, 'epoch': 1.97}
{'loss': 0.0511, 'grad_norm': 0.8468801975250244, 'learning_rate': 2.2147699390090425e-09, 'epoch': 1.98}
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 8
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1205/1210 [6:04:21<01:23, 16.65s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205                                                                                
{'eval_loss': 0.10755157470703125, 'eval_runtime': 31.733, 'eval_samples_per_second': 31.513, 'eval_steps_per_second': 1.008, 'epoch': 1.99}
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-964] due to args.save_total_limit
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1210/1210 [6:06:56<00:00, 21.52s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
{'loss': 0.0452, 'grad_norm': 0.9227968454360962, 'learning_rate': 1.8305224079862015e-11, 'epoch': 2.0}
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210/special_tokens_map.json
/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once


Training completed. Do not forget to share your model on huggingface.co/models =)


/home/wjyeo/anaconda3/envs/ipi_hw/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading best model from /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205 (score: 0.10755157470703125).
Attempting to resume from /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1205
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1210/1210 [6:08:16<00:00, 21.52s/it]Deleting older checkpoint [/mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/checkpoint-1210] due to args.save_total_limit
{'train_runtime': 22099.3536, 'train_samples_per_second': 6.997, 'train_steps_per_second': 0.055, 'train_loss': 0.09583647458021306, 'epoch': 2.0}
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1210/1210 [6:08:27<00:00, 18.27s/it]
Custom impl., saving last checkpoint
 stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Saving model checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/config.json
Configuration saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/generation_config.json
Model weights saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/model.safetensors
chat template saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/chat_template.jinja
tokenizer config file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/tokenizer_config.json
Special tokens file saved in /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/special_tokens_map.json
Processing zero checkpoint '/mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca/global_step1205'
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1525.90it/s]
Detected checkpoint of type zero stage ZeroStageEnum.weights, world_size: 4
Parsing checkpoint created by deepspeed==0.17.6
Gathering sharded weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 1010067.19it/s]
Reconstructed Trainable fp32 state dict with 400 params 8188572672 elements
Saving checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [01:21<00:00, 11.63s/it]
Saved merged FP32 checkpoint to /mnt/disk1/wjyeo/models/Qwen3-8B_sft_ISE_ToolAndAlpaca
